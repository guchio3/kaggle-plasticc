{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## auto encoder を使って欠損を補完する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable #自動微分用\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import erfinv\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#import lightgbm\n",
    "\n",
    "from logging import getLogger\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import argparse\n",
    "import datetime\n",
    "import random\n",
    "from itertools import chain\n",
    "import pickle\n",
    "import warnings\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import gc\n",
    "import sys\n",
    "from multiprocessing import Pool\n",
    "sys.path.append('../')\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEVICE='cpu'\n",
    "DEVICE='cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### まずは biLSTM AE を実装してみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "0. 出力層の acivation, loss func 考える\n",
    "0. 動作確認 \n",
    "0. bilstm に改良 \n",
    "0. three layer に改良 \n",
    "0. dropout, normalization 追加 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NTHREAD = 30\n",
    "random.seed(71)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(71)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderbiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, isCuda, dropout=0.):\n",
    "        super(EncoderbiLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.isCuda = isCuda\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout) #bidirectional=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        #initialize weights\n",
    "        nn.init.xavier_uniform(self.lstm.weight_ih_l0, gain=np.sqrt(2))\n",
    "        nn.init.xavier_uniform(self.lstm.weight_hh_l0, gain=np.sqrt(2))\n",
    "\n",
    "    def forward(self, input):\n",
    "        tt = torch.cuda if self.isCuda == 'cuda' else torch\n",
    "        if self.isCuda == 'cuda':\n",
    "            h0 = tt.FloatTensor(torch.zeros(self.num_layers*2, input.size(0), self.hidden_size).cuda())\n",
    "            c0 = tt.FloatTensor(torch.zeros(self.num_layers*2, input.size(0), self.hidden_size).cuda())\n",
    "        else:\n",
    "            h0 = tt.FloatTensor(torch.zeros(self.num_layers*2, input.size(0), self.hidden_size))\n",
    "            c0 = tt.FloatTensor(torch.zeros(self.num_layers*2, input.size(0), self.hidden_size))\n",
    "        #h0 = Variable(tt.FloatTensor(torch.zeros(self.num_layers*2, input.size(0), self.hidden_size)))\n",
    "        #c0 = Variable(tt.FloatTensor(torch.zeros(self.num_layers*2, input.size(0), self.hidden_size)))\n",
    "        encoded_input, hidden = self.lstm(input, (h0, c0))\n",
    "        encoded_input = self.relu(encoded_input)\n",
    "        return encoded_input\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers, isCuda, dropout=0.):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.isCuda = isCuda\n",
    "        self.lstm = nn.LSTM(hidden_size, output_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        #self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        #initialize weights\n",
    "        nn.init.xavier_uniform(self.lstm.weight_ih_l0, gain=np.sqrt(2))\n",
    "        nn.init.xavier_uniform(self.lstm.weight_hh_l0, gain=np.sqrt(2))\n",
    "        \n",
    "    def forward(self, encoded_input):\n",
    "        #tt = torch.cuda if self.isCuda else torch\n",
    "        tt = torch.cuda if self.isCuda == 'cuda' else torch\n",
    "        if self.isCuda == 'cuda':\n",
    "            h0 = tt.FloatTensor(torch.zeros(self.num_layers, encoded_input.size(0), self.output_size).cuda())\n",
    "            c0 = tt.FloatTensor(torch.zeros(self.num_layers, encoded_input.size(0), self.output_size).cuda())\n",
    "        else:\n",
    "            h0 = tt.FloatTensor(torch.zeros(self.num_layers, encoded_input.size(0), self.output_size))\n",
    "            c0 = tt.FloatTensor(torch.zeros(self.num_layers, encoded_input.size(0), self.output_size))\n",
    "        #h0 = Variable(tt.FloatTensor(torch.zeros(self.num_layers, encoded_input.size(0), self.output_size)))\n",
    "        #c0 = Variable(tt.FloatTensor(torch.zeros(self.num_layers, encoded_input.size(0), self.output_size)))\n",
    "        decoded_output, hidden = self.lstm(encoded_input, (h0, c0))\n",
    "        #decoded_output = self.sigmoid(decoded_output)\n",
    "        decoded_output = decoded_output\n",
    "        return decoded_output\n",
    "\n",
    "class LSTMAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, isCuda, dropout=0.):\n",
    "        super(LSTMAE, self).__init__()\n",
    "        self.encoder = EncoderbiLSTM(input_size, hidden_size, num_layers, isCuda, dropout=dropout)\n",
    "        self.decoder = DecoderRNN(hidden_size*2, input_size, num_layers, isCuda, dropout=dropout)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        encoded_input = self.encoder(input)\n",
    "        decoded_output = self.decoder(encoded_input)\n",
    "        return decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_df = pd.read_csv('/home/naoya.taguchi/.kaggle/competitions/PLAsTiCC-2018/training_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 銀河系外天体に限定\n",
    "train_meta_df = pd.read_csv('/home/naoya.taguchi/.kaggle/competitions/PLAsTiCC-2018/training_set_metadata.csv')[['object_id', 'distmod']]\n",
    "train_set_df = train_set_df.merge(train_meta_df, on='object_id', how='left')\n",
    "train_set_df = train_set_df[train_set_df.distmod.notnull()][['object_id', 'mjd', 'passband', 'flux', 'flux_err']]\n",
    "del train_meta_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_df = pd.read_pickle('/home/naoya.taguchi/.kaggle/competitions/PLAsTiCC-2018/test_set.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 銀河系外天体に限定\n",
    "test_meta_df = pd.read_csv('/home/naoya.taguchi/.kaggle/competitions/PLAsTiCC-2018/test_set_metadata.csv')[['object_id', 'distmod']]\n",
    "test_set_df = test_set_df.merge(test_meta_df, on='object_id', how='left')\n",
    "test_set_df = test_set_df[test_set_df.distmod.notnull()][['object_id', 'mjd', 'passband', 'flux', 'flux_err']]\n",
    "del test_meta_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# サンプリング\n",
    "val_set_df = test_set_df.iloc[30000000:33000000]\n",
    "test_set_df = pd.concat([test_set_df.iloc[:3000000], test_set_df.iloc[20000000:23000000], test_set_df.iloc[-3000000:]], axis=0)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_df = pd.concat([train_set_df, test_set_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passband</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>object_id</th>\n",
       "      <th>int_mjd</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">13</th>\n",
       "      <th>59580</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59581</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59582</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59583</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59584</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59585</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59586</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59587</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59588</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59589</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59590</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59591</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59592</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59593</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59594</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59595</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59596</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59597</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59598</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59599</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59600</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59601</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59602</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59603</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59604</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59605</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59606</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59607</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59608</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59609</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">130755807</th>\n",
       "      <th>60645</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60646</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60647</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60648</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60649</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60650</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60651</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60652</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60653</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60654</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60655</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60656</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60657</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60658</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60659</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60660</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60661</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60662</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60663</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60664</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60665</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60666</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.173342</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60667</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60668</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60669</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60670</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60671</th>\n",
       "      <td>-16.485481</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60672</th>\n",
       "      <td>-7.151317</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60673</th>\n",
       "      <td>-13.423781</td>\n",
       "      <td>2.672931</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60674</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58407567 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "passband                   0         1   2   3   4   5\n",
       "object_id int_mjd                                     \n",
       "13        59580          NaN       NaN NaN NaN NaN NaN\n",
       "          59581          NaN       NaN NaN NaN NaN NaN\n",
       "          59582          NaN       NaN NaN NaN NaN NaN\n",
       "          59583          NaN       NaN NaN NaN NaN NaN\n",
       "          59584          NaN       NaN NaN NaN NaN NaN\n",
       "          59585          NaN       NaN NaN NaN NaN NaN\n",
       "          59586          NaN       NaN NaN NaN NaN NaN\n",
       "          59587          NaN       NaN NaN NaN NaN NaN\n",
       "          59588          NaN       NaN NaN NaN NaN NaN\n",
       "          59589          NaN       NaN NaN NaN NaN NaN\n",
       "          59590          NaN       NaN NaN NaN NaN NaN\n",
       "          59591          NaN       NaN NaN NaN NaN NaN\n",
       "          59592          NaN       NaN NaN NaN NaN NaN\n",
       "          59593          NaN       NaN NaN NaN NaN NaN\n",
       "          59594          NaN       NaN NaN NaN NaN NaN\n",
       "          59595          NaN       NaN NaN NaN NaN NaN\n",
       "          59596          NaN       NaN NaN NaN NaN NaN\n",
       "          59597          NaN       NaN NaN NaN NaN NaN\n",
       "          59598          NaN       NaN NaN NaN NaN NaN\n",
       "          59599          NaN       NaN NaN NaN NaN NaN\n",
       "          59600          NaN       NaN NaN NaN NaN NaN\n",
       "          59601          NaN       NaN NaN NaN NaN NaN\n",
       "          59602          NaN       NaN NaN NaN NaN NaN\n",
       "          59603          NaN       NaN NaN NaN NaN NaN\n",
       "          59604          NaN       NaN NaN NaN NaN NaN\n",
       "          59605          NaN       NaN NaN NaN NaN NaN\n",
       "          59606          NaN       NaN NaN NaN NaN NaN\n",
       "          59607          NaN       NaN NaN NaN NaN NaN\n",
       "          59608          NaN       NaN NaN NaN NaN NaN\n",
       "          59609          NaN       NaN NaN NaN NaN NaN\n",
       "...                      ...       ...  ..  ..  ..  ..\n",
       "130755807 60645          NaN       NaN NaN NaN NaN NaN\n",
       "          60646          NaN       NaN NaN NaN NaN NaN\n",
       "          60647          NaN       NaN NaN NaN NaN NaN\n",
       "          60648          NaN       NaN NaN NaN NaN NaN\n",
       "          60649          NaN       NaN NaN NaN NaN NaN\n",
       "          60650          NaN       NaN NaN NaN NaN NaN\n",
       "          60651          NaN       NaN NaN NaN NaN NaN\n",
       "          60652          NaN       NaN NaN NaN NaN NaN\n",
       "          60653          NaN       NaN NaN NaN NaN NaN\n",
       "          60654          NaN       NaN NaN NaN NaN NaN\n",
       "          60655          NaN       NaN NaN NaN NaN NaN\n",
       "          60656          NaN       NaN NaN NaN NaN NaN\n",
       "          60657          NaN       NaN NaN NaN NaN NaN\n",
       "          60658          NaN       NaN NaN NaN NaN NaN\n",
       "          60659          NaN       NaN NaN NaN NaN NaN\n",
       "          60660          NaN       NaN NaN NaN NaN NaN\n",
       "          60661          NaN       NaN NaN NaN NaN NaN\n",
       "          60662          NaN       NaN NaN NaN NaN NaN\n",
       "          60663          NaN       NaN NaN NaN NaN NaN\n",
       "          60664          NaN       NaN NaN NaN NaN NaN\n",
       "          60665          NaN       NaN NaN NaN NaN NaN\n",
       "          60666          NaN -2.173342 NaN NaN NaN NaN\n",
       "          60667          NaN       NaN NaN NaN NaN NaN\n",
       "          60668          NaN       NaN NaN NaN NaN NaN\n",
       "          60669          NaN       NaN NaN NaN NaN NaN\n",
       "          60670          NaN       NaN NaN NaN NaN NaN\n",
       "          60671   -16.485481       NaN NaN NaN NaN NaN\n",
       "          60672    -7.151317       NaN NaN NaN NaN NaN\n",
       "          60673   -13.423781  2.672931 NaN NaN NaN NaN\n",
       "          60674          NaN       NaN NaN NaN NaN NaN\n",
       "\n",
       "[58407567 rows x 6 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flux, flux_err を学習できる形にする\n",
    "set_df['int_mjd'] = set_df.mjd.astype(int)\n",
    "set_df.drop('mjd', axis=1, inplace=True)\n",
    "gc.collect()\n",
    "pv_set_df = pd.pivot_table(data=set_df, values='flux', index=['object_id', 'int_mjd'], columns='passband', dropna=False, aggfunc='max')\n",
    "pv_set_err_df = pd.pivot_table(data=set_df, values='flux_err', index=['object_id', 'int_mjd'], columns='passband', dropna=False, aggfunc='max')\n",
    "pv_set_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passband</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>object_id</th>\n",
       "      <th>int_mjd</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">111184715</th>\n",
       "      <th>59580</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59581</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59582</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59583</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59584</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59585</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59586</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59587</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59588</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59589</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59590</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59591</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59592</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59593</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59594</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59595</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59596</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59597</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59598</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59599</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59600</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59601</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59602</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59603</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59604</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59605</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59606</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59607</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59608</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59609</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">112175980</th>\n",
       "      <th>60645</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60646</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60647</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60648</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60649</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60650</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60651</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60652</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60653</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60654</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60655</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60656</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60657</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60658</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60659</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60660</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60661</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60662</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60663</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60664</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60665</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60666</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60667</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60668</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60669</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60670</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60671</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60672</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60673</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60674</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22217039 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "passband            0   1   2   3   4   5\n",
       "object_id int_mjd                        \n",
       "111184715 59580   NaN NaN NaN NaN NaN NaN\n",
       "          59581   NaN NaN NaN NaN NaN NaN\n",
       "          59582   NaN NaN NaN NaN NaN NaN\n",
       "          59583   NaN NaN NaN NaN NaN NaN\n",
       "          59584   NaN NaN NaN NaN NaN NaN\n",
       "          59585   NaN NaN NaN NaN NaN NaN\n",
       "          59586   NaN NaN NaN NaN NaN NaN\n",
       "          59587   NaN NaN NaN NaN NaN NaN\n",
       "          59588   NaN NaN NaN NaN NaN NaN\n",
       "          59589   NaN NaN NaN NaN NaN NaN\n",
       "          59590   NaN NaN NaN NaN NaN NaN\n",
       "          59591   NaN NaN NaN NaN NaN NaN\n",
       "          59592   NaN NaN NaN NaN NaN NaN\n",
       "          59593   NaN NaN NaN NaN NaN NaN\n",
       "          59594   NaN NaN NaN NaN NaN NaN\n",
       "          59595   NaN NaN NaN NaN NaN NaN\n",
       "          59596   NaN NaN NaN NaN NaN NaN\n",
       "          59597   NaN NaN NaN NaN NaN NaN\n",
       "          59598   NaN NaN NaN NaN NaN NaN\n",
       "          59599   NaN NaN NaN NaN NaN NaN\n",
       "          59600   NaN NaN NaN NaN NaN NaN\n",
       "          59601   NaN NaN NaN NaN NaN NaN\n",
       "          59602   NaN NaN NaN NaN NaN NaN\n",
       "          59603   NaN NaN NaN NaN NaN NaN\n",
       "          59604   NaN NaN NaN NaN NaN NaN\n",
       "          59605   NaN NaN NaN NaN NaN NaN\n",
       "          59606   NaN NaN NaN NaN NaN NaN\n",
       "          59607   NaN NaN NaN NaN NaN NaN\n",
       "          59608   NaN NaN NaN NaN NaN NaN\n",
       "          59609   NaN NaN NaN NaN NaN NaN\n",
       "...                ..  ..  ..  ..  ..  ..\n",
       "112175980 60645   NaN NaN NaN NaN NaN NaN\n",
       "          60646   NaN NaN NaN NaN NaN NaN\n",
       "          60647   NaN NaN NaN NaN NaN NaN\n",
       "          60648   NaN NaN NaN NaN NaN NaN\n",
       "          60649   NaN NaN NaN NaN NaN NaN\n",
       "          60650   NaN NaN NaN NaN NaN NaN\n",
       "          60651   NaN NaN NaN NaN NaN NaN\n",
       "          60652   NaN NaN NaN NaN NaN NaN\n",
       "          60653   NaN NaN NaN NaN NaN NaN\n",
       "          60654   NaN NaN NaN NaN NaN NaN\n",
       "          60655   NaN NaN NaN NaN NaN NaN\n",
       "          60656   NaN NaN NaN NaN NaN NaN\n",
       "          60657   NaN NaN NaN NaN NaN NaN\n",
       "          60658   NaN NaN NaN NaN NaN NaN\n",
       "          60659   NaN NaN NaN NaN NaN NaN\n",
       "          60660   NaN NaN NaN NaN NaN NaN\n",
       "          60661   NaN NaN NaN NaN NaN NaN\n",
       "          60662   NaN NaN NaN NaN NaN NaN\n",
       "          60663   NaN NaN NaN NaN NaN NaN\n",
       "          60664   NaN NaN NaN NaN NaN NaN\n",
       "          60665   NaN NaN NaN NaN NaN NaN\n",
       "          60666   NaN NaN NaN NaN NaN NaN\n",
       "          60667   NaN NaN NaN NaN NaN NaN\n",
       "          60668   NaN NaN NaN NaN NaN NaN\n",
       "          60669   NaN NaN NaN NaN NaN NaN\n",
       "          60670   NaN NaN NaN NaN NaN NaN\n",
       "          60671   NaN NaN NaN NaN NaN NaN\n",
       "          60672   NaN NaN NaN NaN NaN NaN\n",
       "          60673   NaN NaN NaN NaN NaN NaN\n",
       "          60674   NaN NaN NaN NaN NaN NaN\n",
       "\n",
       "[22217039 rows x 6 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flux, flux_err を学習できる形にする\n",
    "val_set_df['int_mjd'] = val_set_df.mjd.astype(int)\n",
    "val_set_df.drop('mjd', axis=1, inplace=True)\n",
    "gc.collect()\n",
    "pv_val_set_df = pd.pivot_table(data=val_set_df, values='flux', index=['object_id', 'int_mjd'], columns='passband', dropna=False, aggfunc='max')\n",
    "pv_val_set_err_df = pd.pivot_table(data=val_set_df, values='flux_err', index=['object_id', 'int_mjd'], columns='passband', dropna=False, aggfunc='max')\n",
    "pv_val_set_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(pv_set_df)\n",
    "scaled_pv_set_df = pv_set_df.copy()\n",
    "scaled_pv_set_df[[0, 1, 2, 3, 4, 5]] = scaler.transform(pv_set_df)\n",
    "scaled_pv_set_df.fillna(0, inplace=True)\n",
    "\n",
    "scaled_pv_val_set_df = pv_val_set_df.copy()\n",
    "scaled_pv_val_set_df[[0, 1, 2, 3, 4, 5]] = scaler.transform(pv_val_set_df)\n",
    "\n",
    "# err は一旦 scaling しない\n",
    "# 欠損は -1 で表現\n",
    "scaled_pv_set_err_df = pv_set_err_df.copy()\n",
    "scaled_pv_set_err_df.fillna(-1, inplace=True)\n",
    "\n",
    "scaled_pv_val_set_err_df = pv_val_set_err_df.copy()\n",
    "scaled_pv_val_set_err_df.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naoya.taguchi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: \n",
      "Panel is deprecated and will be removed in a future version.\n",
      "The recommended way to represent these types of 3-dimensional data are with a MultiIndex on a DataFrame, via the Panel.to_frame() method\n",
      "Alternatively, you can use the xarray package http://xarray.pydata.org/en/stable/.\n",
      "Pandas provides a `.to_xarray()` method to help automate this conversion.\n",
      "\n",
      "  \n",
      "/home/naoya.taguchi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: \n",
      "Panel is deprecated and will be removed in a future version.\n",
      "The recommended way to represent these types of 3-dimensional data are with a MultiIndex on a DataFrame, via the Panel.to_frame() method\n",
      "Alternatively, you can use the xarray package http://xarray.pydata.org/en/stable/.\n",
      "Pandas provides a `.to_xarray()` method to help automate this conversion.\n",
      "\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/naoya.taguchi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: \n",
      "Panel is deprecated and will be removed in a future version.\n",
      "The recommended way to represent these types of 3-dimensional data are with a MultiIndex on a DataFrame, via the Panel.to_frame() method\n",
      "Alternatively, you can use the xarray package http://xarray.pydata.org/en/stable/.\n",
      "Pandas provides a `.to_xarray()` method to help automate this conversion.\n",
      "\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6, 61417, 951)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset を作るために df -> panel に変換\n",
    "scaled_pv_set_panel = scaled_pv_set_df.to_panel()\n",
    "scaled_pv_set_err_panel = scaled_pv_set_err_df.to_panel()\n",
    "scaled_pv_val_set_panel = scaled_pv_val_set_df.to_panel()\n",
    "scaled_pv_val_set_err_panel = scaled_pv_val_set_df.to_panel()\n",
    "scaled_pv_set_panel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習\n",
    " - 欠損値をどう扱うか\n",
    "     - GPU を使うなら zero_padding が丸いが、スパースだし大体欠損している期間が同じなので欠損期間がうまく出ないのでは...？\n",
    "     - 線形補間とか、何かしらで補完しても良さそう、ただ一方でこれは補完の精度によってきそうだし、良い補完ができるなら別に AE 要らなそう　"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 作り\n",
    "def worker_init_fn(worker_id):                                                          \n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "\n",
    "class plasticcDatasetForAE(Dataset):\n",
    "    def __init__(self, x, y, err):\n",
    "        tt = torch.cuda if DEVICE=='cuda' else torch\n",
    "        self.x = tt.FloatTensor(x.astype(np.float32), device=DEVICE)\n",
    "        #self.x = torch.from_numpy(x.astype(np.float32))\n",
    "        self.y = tt.FloatTensor(y.astype(np.float32), device=DEVICE)\n",
    "        self.err = tt.FloatTensor(err.astype(np.float32), device=DEVICE)\n",
    " \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx, :, :], self.y[idx, :, :], self.err[idx, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 関数を定義\n",
    "# weighted mean square error (https://arxiv.org/abs/1711.10609)\n",
    "\n",
    "def WMSE(true, pred, err):\n",
    "    # 元々欠損値だったところには loss をかけない\n",
    "    mask = (true != -1)#.detach().numpy().astype(np.int64)\n",
    "    mask = mask.float()\n",
    "    n_T = pred.shape[1]\n",
    "    wmse = (1/n_T) * torch.mean(torch.sum((torch.pow(true - pred, 2) / (torch.pow(err, 2))) * mask, dim=(1, 2)))\n",
    "#    wmse = (1/n_T) * torch.mean(torch.sum(torch.sum((torch.pow(true - pred, 2) / (torch.pow(err, 2))) * mask, dim=1), dim=1))\n",
    "#    wmse = (1/n_T) * torch.mean(torch.sum((torch.pow(true - pred, 2) / (torch.pow(err, 2))) * mask, dim=0))\n",
    "#    wmse = (1/n_T) * torch.sum((torch.pow(true - pred, 2) / (torch.pow(err, 2))))\n",
    "    return wmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naoya.taguchi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: \n",
      "Panel is deprecated and will be removed in a future version.\n",
      "The recommended way to represent these types of 3-dimensional data are with a MultiIndex on a DataFrame, via the Panel.to_frame() method\n",
      "Alternatively, you can use the xarray package http://xarray.pydata.org/en/stable/.\n",
      "Pandas provides a `.to_xarray()` method to help automate this conversion.\n",
      "\n",
      "  \n",
      "/home/naoya.taguchi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: \n",
      "Panel is deprecated and will be removed in a future version.\n",
      "The recommended way to represent these types of 3-dimensional data are with a MultiIndex on a DataFrame, via the Panel.to_frame() method\n",
      "Alternatively, you can use the xarray package http://xarray.pydata.org/en/stable/.\n",
      "Pandas provides a `.to_xarray()` method to help automate this conversion.\n",
      "\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/naoya.taguchi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: \n",
      "Panel is deprecated and will be removed in a future version.\n",
      "The recommended way to represent these types of 3-dimensional data are with a MultiIndex on a DataFrame, via the Panel.to_frame() method\n",
      "Alternatively, you can use the xarray package http://xarray.pydata.org/en/stable/.\n",
      "Pandas provides a `.to_xarray()` method to help automate this conversion.\n",
      "\n",
      "  after removing the cwd from sys.path.\n",
      "/home/naoya.taguchi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: \n",
      "Panel is deprecated and will be removed in a future version.\n",
      "The recommended way to represent these types of 3-dimensional data are with a MultiIndex on a DataFrame, via the Panel.to_frame() method\n",
      "Alternatively, you can use the xarray package http://xarray.pydata.org/en/stable/.\n",
      "Pandas provides a `.to_xarray()` method to help automate this conversion.\n",
      "\n",
      "  \"\"\"\n",
      "/home/naoya.taguchi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: \n",
      "Panel is deprecated and will be removed in a future version.\n",
      "The recommended way to represent these types of 3-dimensional data are with a MultiIndex on a DataFrame, via the Panel.to_frame() method\n",
      "Alternatively, you can use the xarray package http://xarray.pydata.org/en/stable/.\n",
      "Pandas provides a `.to_xarray()` method to help automate this conversion.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# data 準備\n",
    "x_trn = scaled_pv_set_panel.swapaxes(0, 1).swapaxes(1, 2).values\n",
    "y_trn = scaled_pv_set_panel.swapaxes(0, 1).swapaxes(1, 2).values\n",
    "x_val = scaled_pv_set_panel.swapaxes(0, 1).swapaxes(1, 2).values\n",
    "y_val = scaled_pv_set_panel.swapaxes(0, 1).swapaxes(1, 2).values\n",
    "err_trn = scaled_pv_set_err_panel.swapaxes(0, 1).swapaxes(1, 2).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((61417, 951, 6), (61417, 951, 6), (61417, 951, 6))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_trn.shape, y_trn.shape, err_trn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naoya.taguchi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \n",
      "/home/naoya.taguchi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/naoya.taguchi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:44: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/home/naoya.taguchi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:45: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015852032\n",
      "0.00414714\n",
      "0.0030047884\n",
      "0.0022173433\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-8e8047af10d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mx_trn_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trn_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_trn_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_trn_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_trn_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_trn_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m#x_trn_batch, y_trn_batch, err_trn_batch = Variable(x_trn_batch), Variable(y_trn_batch), Variable(err_trn_batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbilstm_ae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_trn_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_trn_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_trn_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5253714aa0bf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mencoded_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mdecoded_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecoded_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5253714aa0bf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoded_input)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m#h0 = Variable(tt.FloatTensor(torch.zeros(self.num_layers, encoded_input.size(0), self.output_size)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m#c0 = Variable(tt.FloatTensor(torch.zeros(self.num_layers, encoded_input.size(0), self.output_size)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mdecoded_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;31m#decoded_output = self.sigmoid(decoded_output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mdecoded_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoded_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 179\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "\n",
    "tt = torch.cuda if DEVICE=='cuda' else torch\n",
    "losses = []\n",
    "\n",
    "for i in range(1):\n",
    "    # ae 定義\n",
    "    bilstm_ae = LSTMAE(input_size=6, hidden_size=32, num_layers=3, isCuda=DEVICE, dropout=0.2)\n",
    "    if DEVICE == 'cuda':\n",
    "        bilstm_ae=bilstm_ae.cuda()\n",
    "    # dataset 加工\n",
    "    dataset = plasticcDatasetForAE(x_trn, y_trn, err_trn)\n",
    "    dataloader = DataLoader(dataset, batch_size=1024, shuffle=True, num_workers=0, worker_init_fn=worker_init_fn)\n",
    "    #optimizer = optim.SGD(bilstm_ae.parameters(), lr=0.001, momentum=0.0)\n",
    "    #scheduler = StepLR(optimizer, step_size=5, gamma=0.6)\n",
    "    optimizer = optim.Adam(bilstm_ae.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_losses = []\n",
    "        #scheduler.step()\n",
    "        bilstm_ae = bilstm_ae.train()\n",
    "        for i_batch, sample_batched in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            x_trn_batch, y_trn_batch, err_trn_batch = sample_batched\n",
    "            x_trn_batch, y_trn_batch, err_trn_batch = Variable(tt.FloatTensor(x_trn_batch).cuda()), Variable(tt.FloatTensor(y_trn_batch).cuda()), Variable(tt.FloatTensor(err_trn_batch).cuda())\n",
    "            #x_trn_batch, y_trn_batch, err_trn_batch = Variable(x_trn_batch), Variable(y_trn_batch), Variable(err_trn_batch)\n",
    "            y_pred = bilstm_ae(x_trn_batch)\n",
    "            loss = WMSE(y_trn_batch, y_pred, err_trn_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if DEVICE=='cuda':\n",
    "                batch_loss = loss.cpu().detach().numpy()\n",
    "            else:\n",
    "                batch_loss = loss.detach().numpy()\n",
    "            epoch_losses.append(batch_loss)\n",
    "\n",
    "        bilstm_ae = bilstm_ae.eval()\n",
    "        val_y_pred = bilstm_ae(x_val)\n",
    "        val_loss = WMSE(y_trn_batch, y_pred, err_trn_batch)\n",
    "        epoch_loss = np.mean(epoch_losses)\n",
    "        print(epoch_loss)\n",
    "        losses.append(epoch_loss)\n",
    "        torch.save(bilstm_ae, f'../models/exp1_{i_batch}_{epoch_loss}_{val_loss}.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(y_pred.detach().numpy()).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0307, -0.0775, -0.1336, -0.0143, -0.0987, -0.0795],\n",
      "         [-0.0502, -0.1022, -0.1904, -0.0041, -0.1747, -0.1190],\n",
      "         [-0.0656, -0.1092, -0.2135,  0.0120, -0.2250, -0.1374],\n",
      "         [-0.0776, -0.1089, -0.2206,  0.0269, -0.2561, -0.1466]],\n",
      "\n",
      "        [[-0.0307, -0.0775, -0.1336, -0.0143, -0.0987, -0.0795],\n",
      "         [-0.0502, -0.1022, -0.1904, -0.0041, -0.1747, -0.1190],\n",
      "         [-0.0656, -0.1092, -0.2135,  0.0120, -0.2250, -0.1374],\n",
      "         [-0.0776, -0.1089, -0.2206,  0.0269, -0.2561, -0.1466]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "(tensor([[[-0.0935,  0.2743, -0.0708, -0.0153, -0.1340,  0.3244],\n",
      "         [-0.0935,  0.2743, -0.0708, -0.0153, -0.1340,  0.3244]],\n",
      "\n",
      "        [[-0.0776, -0.1089, -0.2206,  0.0269, -0.2561, -0.1466],\n",
      "         [-0.0776, -0.1089, -0.2206,  0.0269, -0.2561, -0.1466]]],\n",
      "       grad_fn=<ViewBackward>), tensor([[[-0.1542,  0.5232, -0.1357, -0.0332, -0.2322,  0.7198],\n",
      "         [-0.1542,  0.5232, -0.1357, -0.0332, -0.2322,  0.7198]],\n",
      "\n",
      "        [[-0.1403, -0.2042, -0.4817,  0.0422, -0.5078, -0.3620],\n",
      "         [-0.1403, -0.2042, -0.4817,  0.0422, -0.5078, -0.3620]]],\n",
      "       grad_fn=<ViewBackward>))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0307, -0.0775, -0.1336, -0.0143, -0.0987, -0.0795],\n",
       "         [-0.0502, -0.1022, -0.1904, -0.0041, -0.1747, -0.1190],\n",
       "         [-0.0656, -0.1092, -0.2135,  0.0120, -0.2250, -0.1374],\n",
       "         [-0.0776, -0.1089, -0.2206,  0.0269, -0.2561, -0.1466]],\n",
       "\n",
       "        [[-0.0307, -0.0775, -0.1336, -0.0143, -0.0987, -0.0795],\n",
       "         [-0.0502, -0.1022, -0.1904, -0.0041, -0.1747, -0.1190],\n",
       "         [-0.0656, -0.1092, -0.2135,  0.0120, -0.2250, -0.1374],\n",
       "         [-0.0776, -0.1089, -0.2206,  0.0269, -0.2561, -0.1466]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm_ae = LSTMAE(input_size=6, hidden_size=64, num_layers=2, isCuda=False)\n",
    "bilstm_ae(torch.FloatTensor(\n",
    "    np.array(\n",
    "        [[\n",
    "            [0., 1., 3., 2., 4., 5.], \n",
    "            [0., 1., 3., 2., 4., 5.], \n",
    "            [0., 1., 3., 2., 4., 5.], \n",
    "            [0., 1., 3., 2., 4., 5.], \n",
    "        ],\n",
    "         [\n",
    "            [0., 1., 3., 2., 4., 5.], \n",
    "            [0., 1., 3., 2., 4., 5.], \n",
    "            [0., 1., 3., 2., 4., 5.], \n",
    "            [0., 1., 3., 2., 4., 5.], \n",
    "        ],       \n",
    "        ]\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MKRankGaussScalar(object):\n",
    "    \"\"\"usage: \n",
    "    rgs = RankGaussScalar()\n",
    "    rgs.fit(df_X)\n",
    "    df_X_converted = rgs.transform(df_X)\n",
    "    df_X_test_converted = rgs.transform(df_X_test)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.fit_done = False\n",
    "\n",
    "    def rank_gauss(self, x):\n",
    "        N = x.shape[0]\n",
    "        temp = x.argsort()\n",
    "        rank_x = temp.argsort() / N\n",
    "        rank_x -= rank_x.mean()\n",
    "        rank_x *= 2\n",
    "        efi_x = erfinv(rank_x)\n",
    "        efi_x -= efi_x.mean()\n",
    "        return efi_x\n",
    "\n",
    "    def fit(self, df_x):\n",
    "        \"\"\"\n",
    "        df_x: fitting対象のDataFrame\n",
    "        \"\"\"\n",
    "        self.train_unique_rankgauss = {}\n",
    "        self.target_cols = np.sort(df_x.columns)\n",
    "        for c in self.target_cols:\n",
    "            unique_val = np.sort(df_x[c].unique())\n",
    "            self.train_unique_rankgauss[c]= [unique_val, self.rank_gauss(unique_val)]\n",
    "        self.fit_done = True\n",
    "\n",
    "    def transform(self, df_target):\n",
    "        \"\"\"\n",
    "        df_target: transform対象のDataFrame\n",
    "        \"\"\"\n",
    "        assert self.fit_done\n",
    "        assert np.all(np.sort(np.intersect1d(df_target.columns, self.target_cols)) == np.sort(self.target_cols))\n",
    "        df_converted_rank_gauss = pd.DataFrame(index=df_target.index)\n",
    "        for c in self.target_cols:\n",
    "            df_converted_rank_gauss[c] = np.interp(df_target[c], \n",
    "                                                   self.train_unique_rankgauss[c][0], \n",
    "                                                   self.train_unique_rankgauss[c][1]) # ,left=0, right=0)\n",
    "        return df_converted_rank_gauss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

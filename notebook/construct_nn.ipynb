{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn 作って plasiticc で活かしたい\n",
    "やること\n",
    " * 基本的な mlp 作成　& 性能調査\n",
    " * cnn 使った band 間の情報抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import erfinv\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm\n",
    "\n",
    "from logging import getLogger\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import datetime\n",
    "import pickle\n",
    "import warnings\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from tools.my_logging import logInit\n",
    "from tools.feature_tools import feature_engineering\n",
    "from tools.objective_function import weighted_multi_logloss, lgb_multi_weighted_logloss, wloss_objective, wloss_metric, softmax, calc_team_score\n",
    "from tools.model_io import save_models, load_models\n",
    "from tools.fold_resampling import get_fold_resampling_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NTHREAD = 30\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch の tutorial 回してみた"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horse  ship   cat horse\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-62:\n",
      "Process Process-61:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n"
     ]
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.388\n",
      "None\n",
      "tensor([6, 7, 4, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-67:\n",
      "Process Process-68:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-215-f4ac41c3d8e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-211-edbd921bbc5f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/cifar.py\", line 121, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py\", line 49, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py\", line 76, in __call__\n",
      "    return F.to_tensor(pic)\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/functional.py\", line 83, in to_tensor\n",
      "    return img.float().div(255)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            print(loss.grad)\n",
    "            print(labels)\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:    cat  ship  ship plane\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f29a295f2b0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 151, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 494, in Client\n",
      "    deliver_challenge(c, authkey)\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 722, in deliver_challenge\n",
      "    response = connection.recv_bytes(256)        # reject large message\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/naoya.taguchi/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 383, in _recv\n",
      "    raise EOFError\n",
      "EOFError: \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn の作成開始\n",
    "まずは　mlp から。\n",
    " * ネットワーク構造、batchnorm の有無、drop out の値を　configurable に\n",
    " * cnn との連携を意識\n",
    " * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "class plasticcNet(nn.Module):\n",
    "    def __init__(self, mlp_shapes, band_cnn_idxes=None):\n",
    "        super(plasticcNet, self).__init__()\n",
    "        self.activation = torch.nn.ELU()\n",
    "        self.band_cnn_idxes = band_cnn_idxes\n",
    "        # band 軽特徴量間の幾何的構造に基づく関係性を捉える cnn \n",
    "        self.band_conv_nets = []\n",
    "        if band_cnn_idxes:\n",
    "            for band_cnn_idx in band_cnn_idxes:\n",
    "                None\n",
    "            None\n",
    "        self.dense_nets = nn.ModuleList()\n",
    "        #self.dense_nets = []\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        #self.batch_norms = []\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        #self.dropouts = []\n",
    "        # mlp を組む\n",
    "        for i, shape in enumerate(mlp_shapes):\n",
    "            self.dense_nets.append(nn.Linear(shape[0], shape[1]))\n",
    "            #self.batch_norms.append(nn.LayerNorm(shape[1]))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(shape[1]))\n",
    "            if i == 0:\n",
    "                p = 0.00\n",
    "            elif i == len(mlp_shapes) - 1:\n",
    "                p = 0.0\n",
    "            else:\n",
    "                p = 0.0\n",
    "            self.dropouts.append(nn.Dropout(p=p))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        if len(self.band_conv_nets) > 0:\n",
    "            for band_conv_net in self.band_conv_nets:\n",
    "                None\n",
    "        for i, nets in enumerate(list(zip(self.dense_nets, self.batch_norms, self.dropouts))):\n",
    "            dense_net, batch_norm, dropout = nets\n",
    "            if i < len(self.dense_nets) - 1:\n",
    "                #x = torch.relu(dense_net(x))\n",
    "                x = self.activation(dense_net(x))\n",
    "                #ac = torch.nn.LeakyReLU()\n",
    "                #ac = torch.nn.ELU()\n",
    "                #ac = torch.nn.PReLU()\n",
    "                #x = ac(x)\n",
    "                #x = torch.selu(dense_net(x))\n",
    "                #x = torch.tanh(dense_net(x))\n",
    "                x = batch_norm(x)\n",
    "                x = dropout(x)\n",
    "            else:\n",
    "                x =  dense_net(x)\n",
    "                #x = batch_norm(x)\n",
    "                #x = dropout(x)\n",
    "                #x =  torch.softmax(dense_net(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "plasticcNet()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnn = plasticcNet(mlp_shapes=((100, 20), (20, 1)))\n",
    "pnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1152],\n",
       "        [ 0.0955],\n",
       "        [ 0.0000],\n",
       "        [ 0.1930],\n",
       "        [-0.2114],\n",
       "        [-0.1143],\n",
       "        [-0.1207],\n",
       "        [ 0.0140],\n",
       "        [-0.1164],\n",
       "        [ 0.0820],\n",
       "        [ 0.0000],\n",
       "        [-0.0000],\n",
       "        [ 0.0000],\n",
       "        [-0.0961],\n",
       "        [ 0.0558],\n",
       "        [-0.0563],\n",
       "        [ 0.0571],\n",
       "        [-0.2034],\n",
       "        [-0.0281],\n",
       "        [ 0.1225]], grad_fn=<DropoutBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(20, 100)\n",
    "res = pnn(input)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### とりあえずネットワークはできたので loss を定義して学習させてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-23 16:09:53,682 __main__             680 [INFO] [feature_engineering] getting split dfs ... \n",
      "2018-11-23 16:09:53,682 __main__             680 [INFO] [feature_engineering] getting split dfs ... \n",
      "INFO:__main__:getting split dfs ...\n",
      "2018-11-23 16:09:53,685 __main__             38 [INFO] [split_dfs] calculating uniq object_id num \n",
      "2018-11-23 16:09:53,685 __main__             38 [INFO] [split_dfs] calculating uniq object_id num \n",
      "INFO:__main__:calculating uniq object_id num\n",
      "2018-11-23 16:09:53,695 __main__             40 [INFO] [split_dfs] getting groups \n",
      "2018-11-23 16:09:53,695 __main__             40 [INFO] [split_dfs] getting groups \n",
      "INFO:__main__:getting groups\n",
      "2018-11-23 16:09:53,697 __main__             42 [INFO] [split_dfs] splitting df \n",
      "2018-11-23 16:09:53,697 __main__             42 [INFO] [split_dfs] splitting df \n",
      "INFO:__main__:splitting df\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|▋         | 2/30 [00:00<00:01, 14.74it/s]\u001b[A\n",
      " 13%|█▎        | 4/30 [00:00<00:01, 14.87it/s]\u001b[A\n",
      " 20%|██        | 6/30 [00:00<00:01, 14.93it/s]\u001b[A\n",
      " 27%|██▋       | 8/30 [00:00<00:01, 14.98it/s]\u001b[A\n",
      " 33%|███▎      | 10/30 [00:00<00:01, 14.95it/s]\u001b[A\n",
      " 40%|████      | 12/30 [00:00<00:01, 14.94it/s]\u001b[A\n",
      " 47%|████▋     | 14/30 [00:00<00:01, 15.00it/s]\u001b[A\n",
      " 53%|█████▎    | 16/30 [00:01<00:00, 14.97it/s]\u001b[A\n",
      " 60%|██████    | 18/30 [00:01<00:00, 14.98it/s]\u001b[A\n",
      " 67%|██████▋   | 20/30 [00:01<00:00, 15.03it/s]\u001b[A\n",
      " 73%|███████▎  | 22/30 [00:01<00:00, 15.09it/s]\u001b[A\n",
      " 80%|████████  | 24/30 [00:01<00:00, 15.10it/s]\u001b[A\n",
      " 87%|████████▋ | 26/30 [00:01<00:00, 15.09it/s]\u001b[A\n",
      " 93%|█████████▎| 28/30 [00:01<00:00, 15.09it/s]\u001b[A\n",
      "100%|██████████| 30/30 [00:01<00:00, 15.08it/s]\u001b[A\n",
      "\u001b[A2018-11-23 16:09:55,694 __main__             687 [INFO] [feature_engineering] adding corrected flux... \n",
      "2018-11-23 16:09:55,694 __main__             687 [INFO] [feature_engineering] adding corrected flux... \n",
      "INFO:__main__:adding corrected flux...\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:00,  6.19it/s]\u001b[A\n",
      "2it [00:00,  6.33it/s]\u001b[A\n",
      "3it [00:00,  6.37it/s]\u001b[A\n",
      "4it [00:00,  6.45it/s]\u001b[A\n",
      "5it [00:00,  6.47it/s]\u001b[A\n",
      "6it [00:00,  6.48it/s]\u001b[A\n",
      "7it [00:01,  6.48it/s]\u001b[A\n",
      "8it [00:01,  6.51it/s]\u001b[A\n",
      "9it [00:01,  6.57it/s]\u001b[A\n",
      "10it [00:01,  6.62it/s]\u001b[A\n",
      "11it [00:01,  6.63it/s]\u001b[A\n",
      "12it [00:01,  6.66it/s]\u001b[A\n",
      "13it [00:01,  6.66it/s]\u001b[A\n",
      "14it [00:02,  6.67it/s]\u001b[A\n",
      "15it [00:02,  6.68it/s]\u001b[A\n",
      "16it [00:02,  6.71it/s]\u001b[A\n",
      "17it [00:02,  6.72it/s]\u001b[A\n",
      "18it [00:02,  6.73it/s]\u001b[A\n",
      "19it [00:02,  6.74it/s]\u001b[A\n",
      "20it [00:02,  6.76it/s]\u001b[A\n",
      "21it [00:03,  6.77it/s]\u001b[A\n",
      "22it [00:03,  6.79it/s]\u001b[A\n",
      "23it [00:03,  6.80it/s]\u001b[A\n",
      "24it [00:03,  6.82it/s]\u001b[A\n",
      "25it [00:03,  6.82it/s]\u001b[A\n",
      "26it [00:03,  6.83it/s]\u001b[A\n",
      "27it [00:03,  6.84it/s]\u001b[A\n",
      "28it [00:04,  6.84it/s]\u001b[A\n",
      "29it [00:04,  6.85it/s]\u001b[A\n",
      "30it [00:04,  6.86it/s]\u001b[A\n",
      "\u001b[A2018-11-23 16:10:00,076 __main__             692 [INFO] [feature_engineering] start fature engineering ... \n",
      "2018-11-23 16:10:00,076 __main__             692 [INFO] [feature_engineering] start fature engineering ... \n",
      "INFO:__main__:start fature engineering ...\n",
      "2018-11-23 16:10:22,101 __main__             720 [INFO] [feature_engineering] post processing ... \n",
      "2018-11-23 16:10:22,101 __main__             720 [INFO] [feature_engineering] post processing ... \n",
      "INFO:__main__:post processing ...\n"
     ]
    }
   ],
   "source": [
    "# feature を作る\n",
    "logger = getLogger(__name__)\n",
    "logInit(logger, log_dir='../log/', log_filename='notebook.log')\n",
    "\n",
    "training_set_df = pd.read_csv('/home/naoya.taguchi/.kaggle/competitions/PLAsTiCC-2018/training_set.csv')\n",
    "training_set_metadata_df = pd.read_csv('/home/naoya.taguchi/.kaggle/competitions/PLAsTiCC-2018/training_set_metadata.csv')\n",
    "train_df = feature_engineering(\n",
    "        training_set_df,\n",
    "        training_set_metadata_df,\n",
    "        nthread=NTHREAD,\n",
    "        logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 欠損率計算関数\n",
    "def get_null_stat(df):\n",
    "    whole_num = df.shape[0]\n",
    "    res_list = []\n",
    "    for col in df.columns:\n",
    "        res_list.append([col, df[df[col].isnull()].shape[0]/whole_num])\n",
    "    return pd.DataFrame(res_list).rename(columns={0: 'column', 1: 'null_rat'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>null_rat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [column, null_rat]\n",
       "Index: []"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 欠損値が多い... とりあえず　mean で埋めるか...\n",
    "null_df = get_null_stat(train_df).sort_values('null_rat', ascending=False)\n",
    "null_df[null_df.null_rat != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欠損値埋め\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "#for col in tqdm(train_df.columns):\n",
    "#    target_series = train_df[col]\n",
    "#    target_series[target_series.isnull()] = target_series.mean()\n",
    "train_df = train_df.replace(np.inf, np.nan)\n",
    "train_df = train_df.replace(-np.inf, np.nan)\n",
    "train_df = train_df.fillna(train_df.mean())#.drop(null_df[null_df.null_rat != 0]['column'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "class plasticcDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.from_numpy(x.astype(np.float32))\n",
    "#        self.y = torch.from_numpy(y.astype(np.float32))\n",
    "        self.y = torch.LongTensor(y.astype(np.float32))\n",
    "        #self.y = y\n",
    "#        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    " \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        # tolist がないとエラー吐く\n",
    "        return self.x[idx, :], self.y[idx]\n",
    "        #return self.transform(self.x[idx, :]), self.transform(self.y[idx])\n",
    "        #return self.x[idx, :].tolist(), self.y[idx].tolist()\n",
    "\n",
    "\n",
    "    \n",
    "label_num = 14\n",
    "FOLD_NUM = 5\n",
    "le = LabelEncoder()\n",
    "le.fit(train_df['target'].values)\n",
    "x_train = train_df.drop('target', axis=1).values\n",
    "y_train = le.transform(train_df.target)   \n",
    "\n",
    "dataset = plasticcDataset(x_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=250, shuffle=True, num_workers=0)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    x, y = sample_batched\n",
    "    #print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 4.1263962e+01, 3.3065610e+09, ..., 8.6252874e-01,\n",
       "        1.3747127e-01, 1.0000000e+00],\n",
       "       [2.5520000e-01, 4.5406300e+01, 1.2057575e+10, ..., 2.9571021e-01,\n",
       "        7.0428979e-01, 0.0000000e+00],\n",
       "       [1.5699999e-02, 4.0256100e+01, 1.1251748e+09, ..., 8.6330366e-01,\n",
       "        1.3669635e-01, 0.0000000e+00],\n",
       "       ...,\n",
       "       [0.0000000e+00, 4.1263962e+01, 3.3065610e+09, ..., 6.6294086e-01,\n",
       "        3.3705917e-01, 1.0000000e+00],\n",
       "       [0.0000000e+00, 4.1263962e+01, 3.3065610e+09, ..., 8.9639026e-01,\n",
       "        1.0360976e-01, 1.0000000e+00],\n",
       "       [0.0000000e+00, 4.1263962e+01, 3.3065610e+09, ..., 9.3905771e-01,\n",
       "        6.0942307e-02, 1.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_train.dtype)\n",
    "x_train.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252/252 [00:00<00:00, 349.16it/s]\n"
     ]
    }
   ],
   "source": [
    "class GaussRankScaler():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epsilon = 0.001\n",
    "        self.lower = -1 + self.epsilon\n",
    "        self.upper = 1 - self.epsilon\n",
    "        self.range = self.upper - self.lower\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "\n",
    "        i = np.argsort(X, axis=0)\n",
    "        j = np.argsort(i, axis=0)\n",
    "\n",
    "        assert (j.min() == 0).all()\n",
    "        assert (j.max() == len(j) - 1).all()\n",
    "\n",
    "        j_range = len(j) - 1\n",
    "        self.divider = j_range / self.range\n",
    "\n",
    "        transformed = j / self.divider\n",
    "        transformed = transformed - self.upper\n",
    "        transformed = erfinv(transformed)\n",
    "\n",
    "        return transformed\n",
    "    \n",
    "def continuousNormalization(target_df, scaler):\n",
    "    for column in tqdm(target_df.columns.values):\n",
    "        # minmax normalization for continuous data\n",
    "        if target_df[column].dtype != 'object' and column != 'target':\n",
    "            if target_df[column].max() > 0:\n",
    "                target_df[column] = \\\n",
    "                    scaler.fit_transform(target_df[column])\n",
    "    return target_df    \n",
    "    \n",
    "scaler = GaussRankScaler()    \n",
    "train_df = continuousNormalization(train_df, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective\n",
    "# y_h に nan が起こりうる -> classsize が　0 になりうるのでどうにかせねば...\n",
    "# \n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
    "class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "def wloss_metric(preds, train_data):\n",
    "    weight_tensor = torch.tensor(np.array(list(class_weight.values())).astype(np.float), requires_grad=True).type(torch.FloatTensor)\n",
    "    #weight_tensor = torch.tensor(list(class_weight.values()), requires_grad=True).type(torch.FloatTensor)\n",
    "    y_t = torch.tensor(train_data, requires_grad=True).type(torch.LongTensor)\n",
    "    y_h = torch.zeros(y_t.shape[0], len(classes), requires_grad=True).scatter(1, y_t.reshape(-1, 1), 1)\n",
    "    print(y_t.shape)\n",
    "    y_h[y_h == 0] = 1\n",
    "    y_h /= y_h.sum(dim=0, keepdim=True)\n",
    "    y_p = torch.tensor(preds, requires_grad=True).type(torch.FloatTensor)\n",
    "    if len(y_p.shape) == 1:\n",
    "        y_p = y_p.reshape(len(classes), -1).transpose(0, 1)\n",
    "    ln_p = torch.log_softmax(y_p, dim=1)\n",
    "    wll = torch.sum(y_h * ln_p, dim=0)\n",
    "    loss = -torch.dot(weight_tensor, wll) / torch.sum(weight_tensor)\n",
    "    return loss\n",
    "\n",
    "def mywloss(y_pred_raw, y_true):  \n",
    "    weight_tensor = torch.tensor(np.array(list(class_weight.values())).astype(np.float), requires_grad=True).type(torch.FloatTensor)\n",
    "    y_onehot = torch.FloatTensor(y_true.shape[0], len(classes))\n",
    "    y_onehot.zero_()\n",
    "    y_onehot.scatter_(1, y_true.reshape(-1, 1), 1)\n",
    "    \n",
    "#    y_pred=torch.clamp(y_pred,1e-15,1-1e-15)\n",
    "    y_pred_log=torch.log_softmax(y_pred_raw, dim=1)\n",
    "    class_num = torch.sum(y_onehot, dim=0)\n",
    "    class_num[class_num == 0] = 1\n",
    "#    print(y_pred.shape)\n",
    "#    print(y_onehot.shape)\n",
    "#    print(torch.mean(y_onehot*torch.log(y_pred), dim=0).shape)\n",
    "#    print(y_pred)\n",
    "#    print(torch.log(y_pred))\n",
    "#    print(weight_tensor)\n",
    "#    print((class_num.numpy() == 0).any())\n",
    "    loss=-(torch.mean(torch.sum(torch.sum(y_onehot*y_pred_log, dim=0)*weight_tensor/class_num) / torch.sum(weight_tensor)))\n",
    "#            torch.mean(y_onehot*torch.log(y_pred), dim=0)/weight_tensor\n",
    "    return loss\n",
    "\n",
    "def get_l2_loss(model, weight=0.1):\n",
    "    l2_weight = torch.tensor(weight)\n",
    "    l2_reg = torch.tensor(0.)\n",
    "    for param in model.parameters():\n",
    "        l2_reg += torch.norm(param)\n",
    "    return l2_weight * l2_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance_1</th>\n",
       "      <th>importance_2</th>\n",
       "      <th>importance_3</th>\n",
       "      <th>importance_4</th>\n",
       "      <th>importance_5</th>\n",
       "      <th>importance_mean</th>\n",
       "      <th>importance_std</th>\n",
       "      <th>importance_cov</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>band-4_flux_diff</td>\n",
       "      <td>3.877149</td>\n",
       "      <td>110.378603</td>\n",
       "      <td>3.337464</td>\n",
       "      <td>9.850000</td>\n",
       "      <td>1.376004</td>\n",
       "      <td>25.763844</td>\n",
       "      <td>47.406868</td>\n",
       "      <td>1.840054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flux_ratio_sq_kurtosis</td>\n",
       "      <td>4.466670</td>\n",
       "      <td>3.508977</td>\n",
       "      <td>3.271847</td>\n",
       "      <td>1.852178</td>\n",
       "      <td>35.689840</td>\n",
       "      <td>9.757902</td>\n",
       "      <td>14.526536</td>\n",
       "      <td>1.488695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>passband_flux_means_var</td>\n",
       "      <td>10.430280</td>\n",
       "      <td>9.877646</td>\n",
       "      <td>136.335002</td>\n",
       "      <td>9.573529</td>\n",
       "      <td>21.001303</td>\n",
       "      <td>37.443552</td>\n",
       "      <td>55.489190</td>\n",
       "      <td>1.481942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>flux_diff</td>\n",
       "      <td>36.171543</td>\n",
       "      <td>4.911519</td>\n",
       "      <td>0.857851</td>\n",
       "      <td>3.081147</td>\n",
       "      <td>4.871393</td>\n",
       "      <td>9.978691</td>\n",
       "      <td>14.735943</td>\n",
       "      <td>1.476741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>flux_std</td>\n",
       "      <td>108.792653</td>\n",
       "      <td>12.284080</td>\n",
       "      <td>13.534921</td>\n",
       "      <td>15.695708</td>\n",
       "      <td>2.997915</td>\n",
       "      <td>30.661055</td>\n",
       "      <td>43.945350</td>\n",
       "      <td>1.433263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>band-0_detected_mean</td>\n",
       "      <td>1.069664</td>\n",
       "      <td>24.448623</td>\n",
       "      <td>2.925277</td>\n",
       "      <td>4.484955</td>\n",
       "      <td>2.932387</td>\n",
       "      <td>7.172181</td>\n",
       "      <td>9.733311</td>\n",
       "      <td>1.357092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>band_flux_diff_diff</td>\n",
       "      <td>10.967242</td>\n",
       "      <td>33.592161</td>\n",
       "      <td>16.056421</td>\n",
       "      <td>132.825686</td>\n",
       "      <td>7.726280</td>\n",
       "      <td>40.233558</td>\n",
       "      <td>52.714288</td>\n",
       "      <td>1.310207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>band-4_normed_amp</td>\n",
       "      <td>0.807821</td>\n",
       "      <td>0.019941</td>\n",
       "      <td>4.888293</td>\n",
       "      <td>0.093319</td>\n",
       "      <td>2.547364</td>\n",
       "      <td>1.671348</td>\n",
       "      <td>2.066345</td>\n",
       "      <td>1.236335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>band-3_std_upper_flux_count</td>\n",
       "      <td>10.514201</td>\n",
       "      <td>4.516352</td>\n",
       "      <td>22.024038</td>\n",
       "      <td>1.465285</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>7.847505</td>\n",
       "      <td>8.814045</td>\n",
       "      <td>1.123165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>band-3_flux_diff</td>\n",
       "      <td>1.876173</td>\n",
       "      <td>1.459195</td>\n",
       "      <td>12.709589</td>\n",
       "      <td>2.702602</td>\n",
       "      <td>2.530644</td>\n",
       "      <td>4.255641</td>\n",
       "      <td>4.752335</td>\n",
       "      <td>1.116714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>band-5_flux_skew</td>\n",
       "      <td>31.052368</td>\n",
       "      <td>2.871760</td>\n",
       "      <td>4.206042</td>\n",
       "      <td>42.564700</td>\n",
       "      <td>3.744080</td>\n",
       "      <td>16.887790</td>\n",
       "      <td>18.641164</td>\n",
       "      <td>1.103825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>band_flux_diff_max</td>\n",
       "      <td>5.242833</td>\n",
       "      <td>8.592988</td>\n",
       "      <td>0.040430</td>\n",
       "      <td>0.268899</td>\n",
       "      <td>2.319884</td>\n",
       "      <td>3.293007</td>\n",
       "      <td>3.625020</td>\n",
       "      <td>1.100824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>band-5_flux_ratio_sq_skew</td>\n",
       "      <td>38.141927</td>\n",
       "      <td>15.972026</td>\n",
       "      <td>8.506026</td>\n",
       "      <td>2.888697</td>\n",
       "      <td>3.563381</td>\n",
       "      <td>13.814411</td>\n",
       "      <td>14.569808</td>\n",
       "      <td>1.054682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>band-5_std_upper_flux_count</td>\n",
       "      <td>2.110379</td>\n",
       "      <td>0.515934</td>\n",
       "      <td>0.123048</td>\n",
       "      <td>1.739675</td>\n",
       "      <td>0.096325</td>\n",
       "      <td>0.917072</td>\n",
       "      <td>0.944149</td>\n",
       "      <td>1.029525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>band-5_flux_quantile1090_range</td>\n",
       "      <td>16.465405</td>\n",
       "      <td>4.242042</td>\n",
       "      <td>11.272259</td>\n",
       "      <td>9.216878</td>\n",
       "      <td>50.586307</td>\n",
       "      <td>18.356578</td>\n",
       "      <td>18.542342</td>\n",
       "      <td>1.010120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>band-0_normed_amp</td>\n",
       "      <td>9.849317</td>\n",
       "      <td>8.611453</td>\n",
       "      <td>1.201284</td>\n",
       "      <td>29.881464</td>\n",
       "      <td>6.536245</td>\n",
       "      <td>11.215953</td>\n",
       "      <td>10.945840</td>\n",
       "      <td>0.975917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>band-4_flux_ratio_sq_skew</td>\n",
       "      <td>156.738254</td>\n",
       "      <td>148.483466</td>\n",
       "      <td>11.802758</td>\n",
       "      <td>15.723641</td>\n",
       "      <td>40.191662</td>\n",
       "      <td>74.587956</td>\n",
       "      <td>72.109832</td>\n",
       "      <td>0.966776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>band-0_flux_quantile2575_range</td>\n",
       "      <td>3.319201</td>\n",
       "      <td>3.236736</td>\n",
       "      <td>4.096800</td>\n",
       "      <td>1.595967</td>\n",
       "      <td>14.401440</td>\n",
       "      <td>5.330029</td>\n",
       "      <td>5.152181</td>\n",
       "      <td>0.966633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>band-1_flux_ratio_sq_skew</td>\n",
       "      <td>2.432481</td>\n",
       "      <td>119.968722</td>\n",
       "      <td>22.885455</td>\n",
       "      <td>42.782665</td>\n",
       "      <td>42.302156</td>\n",
       "      <td>46.074296</td>\n",
       "      <td>44.519042</td>\n",
       "      <td>0.966245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>flux_by_flux_ratio_sq_sum</td>\n",
       "      <td>2.414897</td>\n",
       "      <td>0.588911</td>\n",
       "      <td>0.764813</td>\n",
       "      <td>2.946035</td>\n",
       "      <td>0.003351</td>\n",
       "      <td>1.343601</td>\n",
       "      <td>1.266520</td>\n",
       "      <td>0.942631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>std_upper_flux_count</td>\n",
       "      <td>9.575806</td>\n",
       "      <td>3.378482</td>\n",
       "      <td>2.898723</td>\n",
       "      <td>2.408276</td>\n",
       "      <td>0.843583</td>\n",
       "      <td>3.820974</td>\n",
       "      <td>3.355034</td>\n",
       "      <td>0.878057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>flux_count</td>\n",
       "      <td>1.642485</td>\n",
       "      <td>2.110547</td>\n",
       "      <td>0.515403</td>\n",
       "      <td>1.875076</td>\n",
       "      <td>6.161367</td>\n",
       "      <td>2.460976</td>\n",
       "      <td>2.157206</td>\n",
       "      <td>0.876565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>band-4_flux_var</td>\n",
       "      <td>2.215989</td>\n",
       "      <td>3.783295</td>\n",
       "      <td>2.846399</td>\n",
       "      <td>11.283874</td>\n",
       "      <td>2.506696</td>\n",
       "      <td>4.527250</td>\n",
       "      <td>3.822802</td>\n",
       "      <td>0.844398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>band-4_wmean</td>\n",
       "      <td>71.106287</td>\n",
       "      <td>138.500497</td>\n",
       "      <td>68.091949</td>\n",
       "      <td>26.609802</td>\n",
       "      <td>4.525939</td>\n",
       "      <td>61.766895</td>\n",
       "      <td>51.302594</td>\n",
       "      <td>0.830584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>flux_w_mean</td>\n",
       "      <td>82.682895</td>\n",
       "      <td>23.603723</td>\n",
       "      <td>1.245397</td>\n",
       "      <td>77.472321</td>\n",
       "      <td>32.319186</td>\n",
       "      <td>43.464705</td>\n",
       "      <td>35.340259</td>\n",
       "      <td>0.813079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>band_flux_max_min_rat</td>\n",
       "      <td>92.774076</td>\n",
       "      <td>104.753450</td>\n",
       "      <td>92.995802</td>\n",
       "      <td>73.417601</td>\n",
       "      <td>348.055547</td>\n",
       "      <td>142.399295</td>\n",
       "      <td>115.513576</td>\n",
       "      <td>0.811195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>band-2_flux_by_flux_ratio_sq_sum</td>\n",
       "      <td>0.414241</td>\n",
       "      <td>0.364601</td>\n",
       "      <td>1.462901</td>\n",
       "      <td>0.172143</td>\n",
       "      <td>0.716056</td>\n",
       "      <td>0.625988</td>\n",
       "      <td>0.506871</td>\n",
       "      <td>0.809713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>band-1_flux_quantile1090_range</td>\n",
       "      <td>45.757127</td>\n",
       "      <td>47.771742</td>\n",
       "      <td>54.006869</td>\n",
       "      <td>168.948241</td>\n",
       "      <td>34.102734</td>\n",
       "      <td>70.117343</td>\n",
       "      <td>55.715554</td>\n",
       "      <td>0.794604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>band-5_normed_mad</td>\n",
       "      <td>2.248890</td>\n",
       "      <td>15.574552</td>\n",
       "      <td>4.582912</td>\n",
       "      <td>7.352292</td>\n",
       "      <td>21.618058</td>\n",
       "      <td>10.275341</td>\n",
       "      <td>8.094917</td>\n",
       "      <td>0.787800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>band-4_flux_quantile2575_range</td>\n",
       "      <td>14.260948</td>\n",
       "      <td>13.185955</td>\n",
       "      <td>4.347024</td>\n",
       "      <td>46.578077</td>\n",
       "      <td>27.599027</td>\n",
       "      <td>21.194206</td>\n",
       "      <td>16.442265</td>\n",
       "      <td>0.775791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>1_minus_2_std</td>\n",
       "      <td>159.794935</td>\n",
       "      <td>161.197674</td>\n",
       "      <td>155.319410</td>\n",
       "      <td>125.241023</td>\n",
       "      <td>122.461381</td>\n",
       "      <td>144.802885</td>\n",
       "      <td>19.274058</td>\n",
       "      <td>0.133105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>flux_kurtosis</td>\n",
       "      <td>56.867722</td>\n",
       "      <td>56.929875</td>\n",
       "      <td>48.177681</td>\n",
       "      <td>42.708740</td>\n",
       "      <td>59.151600</td>\n",
       "      <td>52.767124</td>\n",
       "      <td>7.020310</td>\n",
       "      <td>0.133043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>band-2_normed_amp</td>\n",
       "      <td>15.999689</td>\n",
       "      <td>14.083120</td>\n",
       "      <td>19.691790</td>\n",
       "      <td>16.357304</td>\n",
       "      <td>17.945264</td>\n",
       "      <td>16.815433</td>\n",
       "      <td>2.114876</td>\n",
       "      <td>0.125770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>band-2_flux_diff</td>\n",
       "      <td>6.984615</td>\n",
       "      <td>6.289267</td>\n",
       "      <td>7.281944</td>\n",
       "      <td>6.930305</td>\n",
       "      <td>8.734112</td>\n",
       "      <td>7.244049</td>\n",
       "      <td>0.908167</td>\n",
       "      <td>0.125367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>band-5_flux_diff</td>\n",
       "      <td>66.198972</td>\n",
       "      <td>63.573152</td>\n",
       "      <td>75.873809</td>\n",
       "      <td>60.819872</td>\n",
       "      <td>54.267591</td>\n",
       "      <td>64.146679</td>\n",
       "      <td>7.917235</td>\n",
       "      <td>0.123424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>band-2_detected_mean</td>\n",
       "      <td>8.232429</td>\n",
       "      <td>10.589870</td>\n",
       "      <td>9.645064</td>\n",
       "      <td>10.821852</td>\n",
       "      <td>8.613074</td>\n",
       "      <td>9.580458</td>\n",
       "      <td>1.152948</td>\n",
       "      <td>0.120344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>band-2_flux_max_ratio_to_the_max</td>\n",
       "      <td>110.693311</td>\n",
       "      <td>122.468618</td>\n",
       "      <td>141.471239</td>\n",
       "      <td>142.243872</td>\n",
       "      <td>146.186597</td>\n",
       "      <td>132.612727</td>\n",
       "      <td>15.320296</td>\n",
       "      <td>0.115527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>band-2_flux_quantile90</td>\n",
       "      <td>42.272474</td>\n",
       "      <td>48.824417</td>\n",
       "      <td>35.641005</td>\n",
       "      <td>43.836491</td>\n",
       "      <td>45.767842</td>\n",
       "      <td>43.268446</td>\n",
       "      <td>4.914476</td>\n",
       "      <td>0.113581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>band-5_flux_max_ratio_to_the_max</td>\n",
       "      <td>185.645938</td>\n",
       "      <td>179.191695</td>\n",
       "      <td>146.965409</td>\n",
       "      <td>174.118785</td>\n",
       "      <td>202.015876</td>\n",
       "      <td>177.587541</td>\n",
       "      <td>20.087321</td>\n",
       "      <td>0.113112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>band-2_flux_quantile2575_range</td>\n",
       "      <td>89.745134</td>\n",
       "      <td>110.055413</td>\n",
       "      <td>108.759300</td>\n",
       "      <td>91.558393</td>\n",
       "      <td>87.775916</td>\n",
       "      <td>97.578831</td>\n",
       "      <td>10.890106</td>\n",
       "      <td>0.111603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>2_minus_3_std</td>\n",
       "      <td>50.115624</td>\n",
       "      <td>44.744492</td>\n",
       "      <td>44.861668</td>\n",
       "      <td>52.172445</td>\n",
       "      <td>57.906645</td>\n",
       "      <td>49.960175</td>\n",
       "      <td>5.505970</td>\n",
       "      <td>0.110207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>std_upper_rat</td>\n",
       "      <td>101.869430</td>\n",
       "      <td>94.627943</td>\n",
       "      <td>82.315992</td>\n",
       "      <td>98.167920</td>\n",
       "      <td>81.400435</td>\n",
       "      <td>91.676344</td>\n",
       "      <td>9.326870</td>\n",
       "      <td>0.101737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>band-4_flux_skew</td>\n",
       "      <td>38.527693</td>\n",
       "      <td>45.895766</td>\n",
       "      <td>45.384845</td>\n",
       "      <td>51.463516</td>\n",
       "      <td>44.760677</td>\n",
       "      <td>45.206499</td>\n",
       "      <td>4.595171</td>\n",
       "      <td>0.101648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>band-2_flux_skew</td>\n",
       "      <td>134.817271</td>\n",
       "      <td>170.363561</td>\n",
       "      <td>169.313636</td>\n",
       "      <td>161.377696</td>\n",
       "      <td>144.255427</td>\n",
       "      <td>156.025518</td>\n",
       "      <td>15.804463</td>\n",
       "      <td>0.101294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>band-0_normed_std</td>\n",
       "      <td>8.578552</td>\n",
       "      <td>9.418822</td>\n",
       "      <td>8.079091</td>\n",
       "      <td>7.441191</td>\n",
       "      <td>9.349027</td>\n",
       "      <td>8.573337</td>\n",
       "      <td>0.843001</td>\n",
       "      <td>0.098328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>5_minus_0_wmean</td>\n",
       "      <td>233.579111</td>\n",
       "      <td>262.751621</td>\n",
       "      <td>236.001511</td>\n",
       "      <td>225.353459</td>\n",
       "      <td>199.398681</td>\n",
       "      <td>231.416876</td>\n",
       "      <td>22.745825</td>\n",
       "      <td>0.098289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>4_minus_5_wmean</td>\n",
       "      <td>81.791915</td>\n",
       "      <td>74.712730</td>\n",
       "      <td>83.219490</td>\n",
       "      <td>72.931841</td>\n",
       "      <td>65.887282</td>\n",
       "      <td>75.708652</td>\n",
       "      <td>7.045932</td>\n",
       "      <td>0.093066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>lumi_dist</td>\n",
       "      <td>101.781363</td>\n",
       "      <td>105.542515</td>\n",
       "      <td>120.379520</td>\n",
       "      <td>113.679104</td>\n",
       "      <td>99.003686</td>\n",
       "      <td>108.077238</td>\n",
       "      <td>8.819287</td>\n",
       "      <td>0.081602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>flux_ratio_to_flux_err_min</td>\n",
       "      <td>130.795533</td>\n",
       "      <td>124.524299</td>\n",
       "      <td>133.933644</td>\n",
       "      <td>143.507162</td>\n",
       "      <td>115.274040</td>\n",
       "      <td>129.606936</td>\n",
       "      <td>10.542943</td>\n",
       "      <td>0.081346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>corrected_flux_ratio_sq_skew</td>\n",
       "      <td>185.289659</td>\n",
       "      <td>166.921279</td>\n",
       "      <td>150.886988</td>\n",
       "      <td>160.444183</td>\n",
       "      <td>157.322794</td>\n",
       "      <td>164.172981</td>\n",
       "      <td>13.141669</td>\n",
       "      <td>0.080048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>1_minus_2_wmean</td>\n",
       "      <td>116.444638</td>\n",
       "      <td>109.945908</td>\n",
       "      <td>124.189549</td>\n",
       "      <td>119.322206</td>\n",
       "      <td>133.498716</td>\n",
       "      <td>120.680203</td>\n",
       "      <td>8.826651</td>\n",
       "      <td>0.073141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>hostgal_photoz_err</td>\n",
       "      <td>419.808069</td>\n",
       "      <td>422.585667</td>\n",
       "      <td>394.201017</td>\n",
       "      <td>419.427307</td>\n",
       "      <td>478.746859</td>\n",
       "      <td>426.953784</td>\n",
       "      <td>31.153074</td>\n",
       "      <td>0.072966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>corrected_flux_dif2</td>\n",
       "      <td>1330.582206</td>\n",
       "      <td>1384.313301</td>\n",
       "      <td>1227.394784</td>\n",
       "      <td>1219.353406</td>\n",
       "      <td>1239.376055</td>\n",
       "      <td>1280.203950</td>\n",
       "      <td>73.374450</td>\n",
       "      <td>0.057315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>corrected_flux_by_flux_ratio_sq_skew</td>\n",
       "      <td>327.005574</td>\n",
       "      <td>342.198596</td>\n",
       "      <td>331.869120</td>\n",
       "      <td>372.629386</td>\n",
       "      <td>327.053545</td>\n",
       "      <td>340.151244</td>\n",
       "      <td>19.183152</td>\n",
       "      <td>0.056396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>detected_mjd_get_max_min_diff</td>\n",
       "      <td>2186.079581</td>\n",
       "      <td>2033.344310</td>\n",
       "      <td>2210.252908</td>\n",
       "      <td>2231.565107</td>\n",
       "      <td>2343.331606</td>\n",
       "      <td>2200.914703</td>\n",
       "      <td>111.365548</td>\n",
       "      <td>0.050600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>corrected_flux_w_mean</td>\n",
       "      <td>716.940858</td>\n",
       "      <td>722.364726</td>\n",
       "      <td>660.289996</td>\n",
       "      <td>715.567090</td>\n",
       "      <td>712.019003</td>\n",
       "      <td>705.436335</td>\n",
       "      <td>25.510195</td>\n",
       "      <td>0.036162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>corrected_flux_min</td>\n",
       "      <td>403.075778</td>\n",
       "      <td>392.246764</td>\n",
       "      <td>404.300082</td>\n",
       "      <td>383.617370</td>\n",
       "      <td>417.766902</td>\n",
       "      <td>400.201379</td>\n",
       "      <td>12.960656</td>\n",
       "      <td>0.032385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>band-1_flux_quantile2575_range</td>\n",
       "      <td>815.317421</td>\n",
       "      <td>814.656468</td>\n",
       "      <td>848.765177</td>\n",
       "      <td>813.350093</td>\n",
       "      <td>781.317067</td>\n",
       "      <td>814.681245</td>\n",
       "      <td>23.859276</td>\n",
       "      <td>0.029287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>flux_by_flux_ratio_sq_skew</td>\n",
       "      <td>984.813930</td>\n",
       "      <td>981.468034</td>\n",
       "      <td>964.586678</td>\n",
       "      <td>978.378760</td>\n",
       "      <td>1037.881659</td>\n",
       "      <td>989.425812</td>\n",
       "      <td>28.158412</td>\n",
       "      <td>0.028459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>internal</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  feature  importance_1  importance_2  \\\n",
       "0                        band-4_flux_diff      3.877149    110.378603   \n",
       "1                  flux_ratio_sq_kurtosis      4.466670      3.508977   \n",
       "2                 passband_flux_means_var     10.430280      9.877646   \n",
       "3                               flux_diff     36.171543      4.911519   \n",
       "4                                flux_std    108.792653     12.284080   \n",
       "5                    band-0_detected_mean      1.069664     24.448623   \n",
       "6                     band_flux_diff_diff     10.967242     33.592161   \n",
       "7                       band-4_normed_amp      0.807821      0.019941   \n",
       "8             band-3_std_upper_flux_count     10.514201      4.516352   \n",
       "9                        band-3_flux_diff      1.876173      1.459195   \n",
       "10                       band-5_flux_skew     31.052368      2.871760   \n",
       "11                     band_flux_diff_max      5.242833      8.592988   \n",
       "12              band-5_flux_ratio_sq_skew     38.141927     15.972026   \n",
       "13            band-5_std_upper_flux_count      2.110379      0.515934   \n",
       "14         band-5_flux_quantile1090_range     16.465405      4.242042   \n",
       "15                      band-0_normed_amp      9.849317      8.611453   \n",
       "16              band-4_flux_ratio_sq_skew    156.738254    148.483466   \n",
       "17         band-0_flux_quantile2575_range      3.319201      3.236736   \n",
       "18              band-1_flux_ratio_sq_skew      2.432481    119.968722   \n",
       "19              flux_by_flux_ratio_sq_sum      2.414897      0.588911   \n",
       "20                   std_upper_flux_count      9.575806      3.378482   \n",
       "21                             flux_count      1.642485      2.110547   \n",
       "22                        band-4_flux_var      2.215989      3.783295   \n",
       "23                           band-4_wmean     71.106287    138.500497   \n",
       "24                            flux_w_mean     82.682895     23.603723   \n",
       "25                  band_flux_max_min_rat     92.774076    104.753450   \n",
       "26       band-2_flux_by_flux_ratio_sq_sum      0.414241      0.364601   \n",
       "27         band-1_flux_quantile1090_range     45.757127     47.771742   \n",
       "28                      band-5_normed_mad      2.248890     15.574552   \n",
       "29         band-4_flux_quantile2575_range     14.260948     13.185955   \n",
       "..                                    ...           ...           ...   \n",
       "220                         1_minus_2_std    159.794935    161.197674   \n",
       "221                         flux_kurtosis     56.867722     56.929875   \n",
       "222                     band-2_normed_amp     15.999689     14.083120   \n",
       "223                      band-2_flux_diff      6.984615      6.289267   \n",
       "224                      band-5_flux_diff     66.198972     63.573152   \n",
       "225                  band-2_detected_mean      8.232429     10.589870   \n",
       "226      band-2_flux_max_ratio_to_the_max    110.693311    122.468618   \n",
       "227                band-2_flux_quantile90     42.272474     48.824417   \n",
       "228      band-5_flux_max_ratio_to_the_max    185.645938    179.191695   \n",
       "229        band-2_flux_quantile2575_range     89.745134    110.055413   \n",
       "230                         2_minus_3_std     50.115624     44.744492   \n",
       "231                         std_upper_rat    101.869430     94.627943   \n",
       "232                      band-4_flux_skew     38.527693     45.895766   \n",
       "233                      band-2_flux_skew    134.817271    170.363561   \n",
       "234                     band-0_normed_std      8.578552      9.418822   \n",
       "235                       5_minus_0_wmean    233.579111    262.751621   \n",
       "236                       4_minus_5_wmean     81.791915     74.712730   \n",
       "237                             lumi_dist    101.781363    105.542515   \n",
       "238            flux_ratio_to_flux_err_min    130.795533    124.524299   \n",
       "239          corrected_flux_ratio_sq_skew    185.289659    166.921279   \n",
       "240                       1_minus_2_wmean    116.444638    109.945908   \n",
       "241                    hostgal_photoz_err    419.808069    422.585667   \n",
       "242                   corrected_flux_dif2   1330.582206   1384.313301   \n",
       "243  corrected_flux_by_flux_ratio_sq_skew    327.005574    342.198596   \n",
       "244         detected_mjd_get_max_min_diff   2186.079581   2033.344310   \n",
       "245                 corrected_flux_w_mean    716.940858    722.364726   \n",
       "246                    corrected_flux_min    403.075778    392.246764   \n",
       "247        band-1_flux_quantile2575_range    815.317421    814.656468   \n",
       "248            flux_by_flux_ratio_sq_skew    984.813930    981.468034   \n",
       "249                              internal      0.000000      0.000000   \n",
       "\n",
       "     importance_3  importance_4  importance_5  importance_mean  \\\n",
       "0        3.337464      9.850000      1.376004        25.763844   \n",
       "1        3.271847      1.852178     35.689840         9.757902   \n",
       "2      136.335002      9.573529     21.001303        37.443552   \n",
       "3        0.857851      3.081147      4.871393         9.978691   \n",
       "4       13.534921     15.695708      2.997915        30.661055   \n",
       "5        2.925277      4.484955      2.932387         7.172181   \n",
       "6       16.056421    132.825686      7.726280        40.233558   \n",
       "7        4.888293      0.093319      2.547364         1.671348   \n",
       "8       22.024038      1.465285      0.717647         7.847505   \n",
       "9       12.709589      2.702602      2.530644         4.255641   \n",
       "10       4.206042     42.564700      3.744080        16.887790   \n",
       "11       0.040430      0.268899      2.319884         3.293007   \n",
       "12       8.506026      2.888697      3.563381        13.814411   \n",
       "13       0.123048      1.739675      0.096325         0.917072   \n",
       "14      11.272259      9.216878     50.586307        18.356578   \n",
       "15       1.201284     29.881464      6.536245        11.215953   \n",
       "16      11.802758     15.723641     40.191662        74.587956   \n",
       "17       4.096800      1.595967     14.401440         5.330029   \n",
       "18      22.885455     42.782665     42.302156        46.074296   \n",
       "19       0.764813      2.946035      0.003351         1.343601   \n",
       "20       2.898723      2.408276      0.843583         3.820974   \n",
       "21       0.515403      1.875076      6.161367         2.460976   \n",
       "22       2.846399     11.283874      2.506696         4.527250   \n",
       "23      68.091949     26.609802      4.525939        61.766895   \n",
       "24       1.245397     77.472321     32.319186        43.464705   \n",
       "25      92.995802     73.417601    348.055547       142.399295   \n",
       "26       1.462901      0.172143      0.716056         0.625988   \n",
       "27      54.006869    168.948241     34.102734        70.117343   \n",
       "28       4.582912      7.352292     21.618058        10.275341   \n",
       "29       4.347024     46.578077     27.599027        21.194206   \n",
       "..            ...           ...           ...              ...   \n",
       "220    155.319410    125.241023    122.461381       144.802885   \n",
       "221     48.177681     42.708740     59.151600        52.767124   \n",
       "222     19.691790     16.357304     17.945264        16.815433   \n",
       "223      7.281944      6.930305      8.734112         7.244049   \n",
       "224     75.873809     60.819872     54.267591        64.146679   \n",
       "225      9.645064     10.821852      8.613074         9.580458   \n",
       "226    141.471239    142.243872    146.186597       132.612727   \n",
       "227     35.641005     43.836491     45.767842        43.268446   \n",
       "228    146.965409    174.118785    202.015876       177.587541   \n",
       "229    108.759300     91.558393     87.775916        97.578831   \n",
       "230     44.861668     52.172445     57.906645        49.960175   \n",
       "231     82.315992     98.167920     81.400435        91.676344   \n",
       "232     45.384845     51.463516     44.760677        45.206499   \n",
       "233    169.313636    161.377696    144.255427       156.025518   \n",
       "234      8.079091      7.441191      9.349027         8.573337   \n",
       "235    236.001511    225.353459    199.398681       231.416876   \n",
       "236     83.219490     72.931841     65.887282        75.708652   \n",
       "237    120.379520    113.679104     99.003686       108.077238   \n",
       "238    133.933644    143.507162    115.274040       129.606936   \n",
       "239    150.886988    160.444183    157.322794       164.172981   \n",
       "240    124.189549    119.322206    133.498716       120.680203   \n",
       "241    394.201017    419.427307    478.746859       426.953784   \n",
       "242   1227.394784   1219.353406   1239.376055      1280.203950   \n",
       "243    331.869120    372.629386    327.053545       340.151244   \n",
       "244   2210.252908   2231.565107   2343.331606      2200.914703   \n",
       "245    660.289996    715.567090    712.019003       705.436335   \n",
       "246    404.300082    383.617370    417.766902       400.201379   \n",
       "247    848.765177    813.350093    781.317067       814.681245   \n",
       "248    964.586678    978.378760   1037.881659       989.425812   \n",
       "249      0.000000      0.000000      0.000000         0.000000   \n",
       "\n",
       "     importance_std  importance_cov  \n",
       "0         47.406868        1.840054  \n",
       "1         14.526536        1.488695  \n",
       "2         55.489190        1.481942  \n",
       "3         14.735943        1.476741  \n",
       "4         43.945350        1.433263  \n",
       "5          9.733311        1.357092  \n",
       "6         52.714288        1.310207  \n",
       "7          2.066345        1.236335  \n",
       "8          8.814045        1.123165  \n",
       "9          4.752335        1.116714  \n",
       "10        18.641164        1.103825  \n",
       "11         3.625020        1.100824  \n",
       "12        14.569808        1.054682  \n",
       "13         0.944149        1.029525  \n",
       "14        18.542342        1.010120  \n",
       "15        10.945840        0.975917  \n",
       "16        72.109832        0.966776  \n",
       "17         5.152181        0.966633  \n",
       "18        44.519042        0.966245  \n",
       "19         1.266520        0.942631  \n",
       "20         3.355034        0.878057  \n",
       "21         2.157206        0.876565  \n",
       "22         3.822802        0.844398  \n",
       "23        51.302594        0.830584  \n",
       "24        35.340259        0.813079  \n",
       "25       115.513576        0.811195  \n",
       "26         0.506871        0.809713  \n",
       "27        55.715554        0.794604  \n",
       "28         8.094917        0.787800  \n",
       "29        16.442265        0.775791  \n",
       "..              ...             ...  \n",
       "220       19.274058        0.133105  \n",
       "221        7.020310        0.133043  \n",
       "222        2.114876        0.125770  \n",
       "223        0.908167        0.125367  \n",
       "224        7.917235        0.123424  \n",
       "225        1.152948        0.120344  \n",
       "226       15.320296        0.115527  \n",
       "227        4.914476        0.113581  \n",
       "228       20.087321        0.113112  \n",
       "229       10.890106        0.111603  \n",
       "230        5.505970        0.110207  \n",
       "231        9.326870        0.101737  \n",
       "232        4.595171        0.101648  \n",
       "233       15.804463        0.101294  \n",
       "234        0.843001        0.098328  \n",
       "235       22.745825        0.098289  \n",
       "236        7.045932        0.093066  \n",
       "237        8.819287        0.081602  \n",
       "238       10.542943        0.081346  \n",
       "239       13.141669        0.080048  \n",
       "240        8.826651        0.073141  \n",
       "241       31.153074        0.072966  \n",
       "242       73.374450        0.057315  \n",
       "243       19.183152        0.056396  \n",
       "244      111.365548        0.050600  \n",
       "245       25.510195        0.036162  \n",
       "246       12.960656        0.032385  \n",
       "247       23.859276        0.029287  \n",
       "248       28.158412        0.028459  \n",
       "249        0.000000             NaN  \n",
       "\n",
       "[250 rows x 9 columns]"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats_df = pd.read_csv('../importances/Booster_weight-multi-logloss-0.579991_2018-11-20-13-16-50_importance.csv')\n",
    "feats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 train_loss : 0.89810 val_loss : 0.98218\n",
      "epoch : 1 train_loss : 0.64145 val_loss : 1.00868\n",
      "epoch : 2 train_loss : 0.59204 val_loss : 0.75442\n",
      "epoch : 3 train_loss : 0.55109 val_loss : 0.89404\n",
      "epoch : 4 train_loss : 0.51872 val_loss : 0.69196\n",
      "epoch : 5 train_loss : 0.49274 val_loss : 0.69753\n",
      "epoch : 6 train_loss : 0.46527 val_loss : 0.72533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 7 train_loss : 0.45909 val_loss : 0.66471\n",
      "epoch : 8 train_loss : 0.44272 val_loss : 0.70301\n",
      "epoch : 9 train_loss : 0.40647 val_loss : 0.87094\n",
      "epoch : 10 train_loss : 0.41511 val_loss : 0.70822\n",
      "epoch : 11 train_loss : 0.41364 val_loss : 0.70921\n",
      "epoch : 12 train_loss : 0.39742 val_loss : 0.65918\n",
      "epoch : 13 train_loss : 0.37302 val_loss : 0.71926\n",
      "epoch : 14 train_loss : 0.37081 val_loss : 0.68081\n",
      "epoch : 15 train_loss : 0.35368 val_loss : 0.74590\n",
      "epoch : 16 train_loss : 0.35344 val_loss : 0.72723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [00:20<01:23, 20.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 17 train_loss : 0.34377 val_loss : 0.66840\n",
      "best loss is 0.6591775417327881\n",
      "epoch : 0 train_loss : 0.89500 val_loss : 0.93411\n",
      "epoch : 1 train_loss : 0.66770 val_loss : 0.66969\n",
      "epoch : 2 train_loss : 0.60136 val_loss : 0.68741\n",
      "epoch : 3 train_loss : 0.53542 val_loss : 0.65454\n",
      "epoch : 4 train_loss : 0.50072 val_loss : 0.66033\n",
      "epoch : 5 train_loss : 0.50958 val_loss : 0.60244\n",
      "epoch : 6 train_loss : 0.46729 val_loss : 0.62915\n",
      "epoch : 7 train_loss : 0.47485 val_loss : 0.61905\n",
      "epoch : 8 train_loss : 0.44024 val_loss : 0.60604\n",
      "epoch : 9 train_loss : 0.43635 val_loss : 0.55742\n",
      "epoch : 10 train_loss : 0.42069 val_loss : 0.62321\n",
      "epoch : 11 train_loss : 0.43233 val_loss : 0.86700\n",
      "epoch : 12 train_loss : 0.40271 val_loss : 0.58627\n",
      "epoch : 13 train_loss : 0.38839 val_loss : 0.60169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [00:39<00:58, 19.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 14 train_loss : 0.37493 val_loss : 0.57820\n",
      "best loss is 0.557422935962677\n",
      "epoch : 0 train_loss : 0.85562 val_loss : 0.96406\n",
      "epoch : 1 train_loss : 0.66287 val_loss : 0.79310\n",
      "epoch : 2 train_loss : 0.57696 val_loss : 0.79293\n",
      "epoch : 3 train_loss : 0.54382 val_loss : 0.65984\n",
      "epoch : 4 train_loss : 0.53063 val_loss : 0.70173\n",
      "epoch : 5 train_loss : 0.48098 val_loss : 0.78938\n",
      "epoch : 6 train_loss : 0.47295 val_loss : 0.71553\n",
      "epoch : 7 train_loss : 0.47611 val_loss : 0.63609\n",
      "epoch : 8 train_loss : 0.43150 val_loss : 0.67333\n",
      "epoch : 9 train_loss : 0.42373 val_loss : 0.68208\n",
      "epoch : 10 train_loss : 0.41132 val_loss : 0.69656\n",
      "epoch : 11 train_loss : 0.39395 val_loss : 0.70224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [00:54<00:36, 18.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 12 train_loss : 0.37414 val_loss : 0.75120\n",
      "best loss is 0.6360897421836853\n",
      "epoch : 0 train_loss : 0.88682 val_loss : 0.88442\n",
      "epoch : 1 train_loss : 0.66016 val_loss : 0.94769\n",
      "epoch : 2 train_loss : 0.56941 val_loss : 0.75752\n",
      "epoch : 3 train_loss : 0.53643 val_loss : 0.68476\n",
      "epoch : 4 train_loss : 0.51103 val_loss : 0.78328\n",
      "epoch : 5 train_loss : 0.49472 val_loss : 0.66924\n",
      "epoch : 6 train_loss : 0.47353 val_loss : 0.72918\n",
      "epoch : 7 train_loss : 0.45673 val_loss : 0.66322\n",
      "epoch : 8 train_loss : 0.43341 val_loss : 0.65665\n",
      "epoch : 9 train_loss : 0.43955 val_loss : 0.74422\n",
      "epoch : 10 train_loss : 0.40797 val_loss : 0.69635\n",
      "epoch : 11 train_loss : 0.39599 val_loss : 0.71004\n",
      "epoch : 12 train_loss : 0.37907 val_loss : 0.68441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [01:10<00:17, 17.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 13 train_loss : 0.36269 val_loss : 0.71234\n",
      "best loss is 0.6566498279571533\n",
      "epoch : 0 train_loss : 0.86572 val_loss : 0.96332\n",
      "epoch : 1 train_loss : 0.64800 val_loss : 0.94876\n",
      "epoch : 2 train_loss : 0.57658 val_loss : 0.77922\n",
      "epoch : 3 train_loss : 0.53851 val_loss : 0.85975\n",
      "epoch : 4 train_loss : 0.49887 val_loss : 0.80263\n",
      "epoch : 5 train_loss : 0.48082 val_loss : 0.81954\n",
      "epoch : 6 train_loss : 0.45561 val_loss : 0.75541\n",
      "epoch : 7 train_loss : 0.46263 val_loss : 0.75825\n",
      "epoch : 8 train_loss : 0.44775 val_loss : 0.81905\n",
      "epoch : 9 train_loss : 0.43497 val_loss : 0.79821\n",
      "epoch : 10 train_loss : 0.41057 val_loss : 0.86613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 5/5 [01:24<00:00, 16.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 11 train_loss : 0.41286 val_loss : 0.78050\n",
      "best loss is 0.75540691614151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "label_num = 14\n",
    "FOLD_NUM = 5\n",
    "le = LabelEncoder()\n",
    "le.fit(train_df['target'].values)\n",
    "\n",
    "# remove features\n",
    "#pd.read_csv\n",
    "\n",
    "#x_train = train_df[[]].drop('target', axis=1).values\n",
    "x_train = train_df.drop(list(feats_df.feature.tolist())[:170], axis=1).drop('target', axis=1).values\n",
    "y_train = le.transform(train_df.target)\n",
    "\n",
    "ss = StandardScaler()\n",
    "scaler = GaussRankScaler()    \n",
    "#x_train = ss.fit_transform(x_train)\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=FOLD_NUM, shuffle=True, random_state=71)\n",
    "folds = skf.split(x_train, y_train)\n",
    "max_layer_size = 1024\n",
    "#max_layer_size = 2048\n",
    "#max_layer_size = 4096\n",
    "#max_layer_size = 512\n",
    "#max_layer_size = 128\n",
    "#max_layer_size = 64\n",
    "#max_layer_size = 256\n",
    "\n",
    "\n",
    "best_models = []\n",
    "best_scores = []\n",
    "for trn_idx, val_idx in tqdm(list(folds)):\n",
    "    x_trn, x_val = x_train[trn_idx], x_train[val_idx]\n",
    "    y_trn, y_val = y_train[trn_idx], y_train[val_idx]\n",
    "    mlp_shapes = (\n",
    "        (x_trn.shape[1], max_layer_size), \n",
    "#        (max_layer_size, max_layer_size//4), \n",
    "#        (max_layer_size//4, max_layer_size//16), \n",
    "        (max_layer_size, max_layer_size//2), \n",
    "        (max_layer_size//2, max_layer_size//4),\n",
    "        (max_layer_size//4, label_num))\n",
    "#        (max_layer_size//16, max_layer_size//32),\n",
    "#        (max_layer_size//32, label_num))\n",
    "#        (max_layer_size, max_layer_size//32), \n",
    "#        (max_layer_size//32, label_num))\n",
    "#        (max_layer_size, max_layer_size//2), \n",
    "#        (max_layer_size//2, label_num))\n",
    "#        (max_layer_size//4, max_layer_size//8), \n",
    "#        (max_layer_size//8, label_num))\n",
    "#        (max_layer_size//4, max_layer_size//8), \n",
    "#        (max_layer_size//8, label_num))\n",
    "#        (max_layer_size//8, max_layer_size//16),\n",
    "#        (max_layer_size//16, label_num))\n",
    "\n",
    "    pnn = plasticcNet(mlp_shapes=mlp_shapes)\n",
    "    optimizer = optim.SGD(pnn.parameters(), lr=0.3, )\n",
    "    #optimizer = optim.ASGD(pnn.parameters(), lr=0.3,  )\n",
    "    #optimizer = optim.Adam(pnn.parameters(), lr=0.01)\n",
    "    #optimizer = optim.RMSprop(pnn.parameters(), lr=0.01)   \n",
    "        \n",
    "    dataset = plasticcDataset(x_trn, y_trn)\n",
    "    dataloader = DataLoader(dataset, batch_size=50, shuffle=True, num_workers=0)\n",
    "\n",
    "    epochs = 1000\n",
    "    losses = []\n",
    "    best_loss = 100000000\n",
    "    best_count = 0\n",
    "    best_model = None   \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i_batch, sample_batched in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            x_trn_batch, y_trn_batch = sample_batched\n",
    "            _res = pnn(x_trn_batch)\n",
    "            #loss = criterion(_res, y_trn_batch)\n",
    "            loss = mywloss(_res, y_trn_batch)\n",
    "            #loss += get_l2_loss(pnn)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.detach().numpy())\n",
    "            #break\n",
    "        val_res = pnn(torch.Tensor(x_val))\n",
    "        val_score = mywloss(val_res, torch.LongTensor(y_val)).detach().numpy()\n",
    "        if epoch % 1 == 0:\n",
    "            if best_loss > val_score:\n",
    "                best_loss = val_score\n",
    "                best_count = 0\n",
    "                best_model = pnn\n",
    "            else:\n",
    "                best_count += 1\n",
    "            print(f'epoch : {epoch}', f'train_loss : {np.mean(losses):.5f}', f'val_loss : {val_score:.5f}')\n",
    "            losses = []\n",
    "            if best_count >= 5:\n",
    "                print(f'best loss is {best_loss}')\n",
    "                best_models.append(best_model)\n",
    "                best_val_res = pnn(torch.Tensor(x_val))\n",
    "                best_val_score = mywloss(best_val_res, torch.LongTensor(y_val)).detach().numpy()\n",
    "                best_scores.append(best_loss)\n",
    "                break\n",
    "        # break\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[plasticcNet(\n",
       "   (activation): ELU(alpha=1.0)\n",
       "   (dense_nets): ModuleList(\n",
       "     (0): Linear(in_features=81, out_features=1024, bias=True)\n",
       "     (1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "     (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "     (3): Linear(in_features=256, out_features=14, bias=True)\n",
       "   )\n",
       "   (batch_norms): ModuleList(\n",
       "     (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (3): BatchNorm1d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (dropouts): ModuleList(\n",
       "     (0): Dropout(p=0.0)\n",
       "     (1): Dropout(p=0.0)\n",
       "     (2): Dropout(p=0.0)\n",
       "     (3): Dropout(p=0.0)\n",
       "   )\n",
       " ), plasticcNet(\n",
       "   (activation): ELU(alpha=1.0)\n",
       "   (dense_nets): ModuleList(\n",
       "     (0): Linear(in_features=81, out_features=1024, bias=True)\n",
       "     (1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "     (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "     (3): Linear(in_features=256, out_features=14, bias=True)\n",
       "   )\n",
       "   (batch_norms): ModuleList(\n",
       "     (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (3): BatchNorm1d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (dropouts): ModuleList(\n",
       "     (0): Dropout(p=0.0)\n",
       "     (1): Dropout(p=0.0)\n",
       "     (2): Dropout(p=0.0)\n",
       "     (3): Dropout(p=0.0)\n",
       "   )\n",
       " ), plasticcNet(\n",
       "   (activation): ELU(alpha=1.0)\n",
       "   (dense_nets): ModuleList(\n",
       "     (0): Linear(in_features=81, out_features=1024, bias=True)\n",
       "     (1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "     (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "     (3): Linear(in_features=256, out_features=14, bias=True)\n",
       "   )\n",
       "   (batch_norms): ModuleList(\n",
       "     (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (3): BatchNorm1d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (dropouts): ModuleList(\n",
       "     (0): Dropout(p=0.0)\n",
       "     (1): Dropout(p=0.0)\n",
       "     (2): Dropout(p=0.0)\n",
       "     (3): Dropout(p=0.0)\n",
       "   )\n",
       " ), plasticcNet(\n",
       "   (activation): ELU(alpha=1.0)\n",
       "   (dense_nets): ModuleList(\n",
       "     (0): Linear(in_features=81, out_features=1024, bias=True)\n",
       "     (1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "     (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "     (3): Linear(in_features=256, out_features=14, bias=True)\n",
       "   )\n",
       "   (batch_norms): ModuleList(\n",
       "     (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (3): BatchNorm1d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (dropouts): ModuleList(\n",
       "     (0): Dropout(p=0.0)\n",
       "     (1): Dropout(p=0.0)\n",
       "     (2): Dropout(p=0.0)\n",
       "     (3): Dropout(p=0.0)\n",
       "   )\n",
       " ), plasticcNet(\n",
       "   (activation): ELU(alpha=1.0)\n",
       "   (dense_nets): ModuleList(\n",
       "     (0): Linear(in_features=81, out_features=1024, bias=True)\n",
       "     (1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "     (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "     (3): Linear(in_features=256, out_features=14, bias=True)\n",
       "   )\n",
       "   (batch_norms): ModuleList(\n",
       "     (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (3): BatchNorm1d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (dropouts): ModuleList(\n",
       "     (0): Dropout(p=0.0)\n",
       "     (1): Dropout(p=0.0)\n",
       "     (2): Dropout(p=0.0)\n",
       "     (3): Dropout(p=0.0)\n",
       "   )\n",
       " )]"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnn = plasticcNet(mlp_shapes=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4671, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.Tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "target = torch.LongTensor([0])\n",
    "mywloss(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wloss_metric(preds, train_data):\n",
    "    classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
    "    class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "    #np.clip(preds, 1e-15,1-1e-15)\n",
    "    weight_tensor = torch.tensor(list(class_weight.values()),\n",
    "                             requires_grad=False).type(torch.FloatTensor)\n",
    "    y_t = torch.tensor(train_data, requires_grad=False).type(torch.LongTensor)\n",
    "    y_h = torch.zeros(\n",
    "        y_t.shape[0], len(classes), requires_grad=False).scatter(1, y_t.reshape(-1, 1), 1)\n",
    "    y_h[y_h == 0] = 1\n",
    "    y_h /= y_h.sum(dim=0, keepdim=True)\n",
    "    y_p = torch.tensor(preds, requires_grad=False).type(torch.FloatTensor)\n",
    "    if len(y_p.shape) == 1:\n",
    "        y_p = y_p.reshape(len(classes), -1).transpose(0, 1)\n",
    "    ln_p = torch.log_softmax(y_p, dim=1)\n",
    "    wll = torch.sum(y_h * ln_p, dim=0)\n",
    "    loss = -torch.dot(weight_tensor, wll) / torch.sum(weight_tensor)\n",
    "    return 'wloss', loss * 1., False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('wloss', 2.62982439994812, False)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pred = np.array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "#target = np.array([0])\n",
    "wloss_metric(pred, target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

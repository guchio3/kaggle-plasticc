{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naoya.taguchi/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable #自動微分用\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import erfinv\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm\n",
    "\n",
    "from logging import getLogger\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import argparse\n",
    "import datetime\n",
    "import random\n",
    "from itertools import chain\n",
    "import pickle\n",
    "import warnings\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import gc\n",
    "import sys\n",
    "from multiprocessing import Pool\n",
    "sys.path.append('../')\n",
    "\n",
    "from tools.my_logging import logInit\n",
    "from tools.feature_tools import feature_engineering\n",
    "from tools.objective_function import weighted_multi_logloss, lgb_multi_weighted_logloss, wloss_objective, wloss_metric, softmax, calc_team_score\n",
    "from tools.model_io import save_models, load_models\n",
    "from tools.fold_resampling import get_fold_resampling_dict\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NTHREAD = 62\n",
    "random.seed(71)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(71)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class plasticcNet(nn.Module):\n",
    "    def __init__(self, input_unit_num, mlp_unit_nums, band_cnn_idxes=None,\n",
    "                init_do_rate=0., middle_do_rate=0.5, last_do_rate=0.25):\n",
    "        super(plasticcNet, self).__init__()\n",
    "        self.activation = torch.nn.ELU()\n",
    "        self.cnn_activation = torch.nn.ELU()\n",
    "        self.cnn_dropout = torch.nn.Dropout(p=0.2)\n",
    "        self.cnn_dropout2 = torch.nn.Dropout(p=0.2)\n",
    "        self.band_cnn_idxes = band_cnn_idxes\n",
    "        self.init_do_rate = init_do_rate\n",
    "        self.middle_do_rate = middle_do_rate\n",
    "        self.last_do_rate = last_do_rate\n",
    "         \n",
    "        # band 軽特徴量間の幾何的構造に基づく関係性を捉える cnn \n",
    "        # 1st では poolig せず、second で　pooling する？\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=3, stride=1)\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size=2, stride=1)\n",
    "        self.band_conv_nets1 = nn.ModuleList()\n",
    "        self.band_conv_nets2 = nn.ModuleList()\n",
    "        self.band_batch_norm1 = nn.ModuleList()\n",
    "        self.band_batch_norm2 = nn.ModuleList()\n",
    "        #self.band_dropout1 = nn.ModuleList()\n",
    "        #self.band_dropout2 = nn.ModuleList()\n",
    "        if self.band_cnn_idxes:\n",
    "            for band_cnn_idx in band_cnn_idxes:\n",
    "                self.band_conv_nets1.append(\n",
    "                    nn.Conv1d(\n",
    "                        in_channels=1,\n",
    "                        out_channels=1,\n",
    "                        kernel_size=4,\n",
    "                        stride=1)\n",
    "                )\n",
    "                #self.band_batch_norm1.append(nn.LayerNorm(6))\n",
    "                self.band_batch_norm1.append(nn.BatchNorm1d(1))\n",
    "                #self.band_dropout1.append()\n",
    "                self.band_conv_nets2.append(\n",
    "                    nn.Conv1d(\n",
    "                        in_channels=1,\n",
    "                        out_channels=1,\n",
    "                        kernel_size=3,\n",
    "                        stride=1)\n",
    "                )\n",
    "                #self.band_batch_norm2.append(nn.LayerNorm(4))\n",
    "                self.band_batch_norm2.append(nn.BatchNorm1d(1))\n",
    "                #self.band_dropout2.append()\n",
    "            None\n",
    "       \n",
    "        # mlp を組む\n",
    "        # 一層目の unit num を作る\n",
    "        if self.band_cnn_idxes:\n",
    "            self.unpacked_band_cnn_idxes = list(chain.from_iterable(self.band_cnn_idxes))\n",
    "            #past_unit_num = input_unit_num + len(self.unpacked_band_cnn_idxes) // 6 * 4 - len(self.unpacked_band_cnn_idxes) # only 1 layer\n",
    "            #past_unit_num = input_unit_num + len(self.unpacked_band_cnn_idxes) // 6 * 6 - len(self.unpacked_band_cnn_idxes) # only 1 layer\n",
    "            self.cnn_out_unit_num = len(self.unpacked_band_cnn_idxes) // 6 * 4\n",
    "            self.cnn_bn = nn.LayerNorm(self.cnn_out_unit_num)\n",
    "            #self.cnn_bn = nn.BatchNorm1d(self.cnn_out_unit_num)\n",
    "            past_unit_num = input_unit_num + self.cnn_out_unit_num - len(self.unpacked_band_cnn_idxes) # w/o pooling\n",
    "            #past_unit_num = input_unit_num + len(self.unpacked_band_cnn_idxes) // 6 * 3 - len(self.unpacked_band_cnn_idxes)\n",
    "        else:\n",
    "            past_unit_num = input_unit_num\n",
    "        self.dense_nets = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        for i, mlp_unit_num in enumerate(mlp_unit_nums):\n",
    "            self.dense_nets.append(nn.Linear(past_unit_num, mlp_unit_num))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(mlp_unit_num))\n",
    "            if i == 0:\n",
    "                p = self.init_do_rate\n",
    "            elif i == len(mlp_unit_nums) - 1:\n",
    "                p = self.last_do_rate\n",
    "            else:\n",
    "                p = self.middle_do_rate\n",
    "            self.dropouts.append(nn.Dropout(p=p))\n",
    "            past_unit_num = mlp_unit_num\n",
    "            \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x_cnns = []\n",
    "        if len(self.band_conv_nets1) > 0:\n",
    "            for zipped in zip(self.band_cnn_idxes, \n",
    "                            self.band_conv_nets1, self.band_batch_norm1, \n",
    "                            self.band_conv_nets2, self.band_batch_norm2):\n",
    "                band_cnn_idx, band_conv_net1, band_batch_norm1, band_conv_net2, band_batch_norm2 = zipped\n",
    "                x_cnn = x[:, band_cnn_idx]\n",
    "                x_cnn = torch.cat([x_cnn, x_cnn[:,:3]], dim=1) # mod\n",
    "                x_cnn = x_cnn.view(batch_size, 1, -1) \n",
    "                x_cnn = band_conv_net1(x_cnn) \n",
    "                x_cnn = self.cnn_activation(x_cnn)\n",
    "                x_cnn = band_batch_norm1(x_cnn)\n",
    "                x_cnn = self.cnn_dropout(x_cnn)\n",
    "                #x_cnn = self.maxpool1(x_cnn)\n",
    "                x_cnn = band_conv_net2(x_cnn) \n",
    "                x_cnn = self.cnn_activation(x_cnn)\n",
    "                #x_cnn = band_batch_norm2(x_cnn)\n",
    "                #x_cnn = self.cnn_dropout(x_cnn)\n",
    "                #x_cnn = self.maxpool2(x_cnn)\n",
    "                x_cnns.append(x_cnn)\n",
    "\n",
    "            x_cnns = torch.cat(x_cnns, dim=1).view(batch_size, -1) # concat して channel を押しつぶす\n",
    "            x_cnns = self.cnn_bn(x_cnns)\n",
    "            x_cnns = self.cnn_dropout2(x_cnns)\n",
    "            x = np.delete(x, self.unpacked_band_cnn_idxes, axis=1) # drop used idxes  self.unpacked....\n",
    "            x = torch.cat([x, x_cnns], dim=1) \n",
    "        else:\n",
    "            x = x\n",
    "        for i, nets in enumerate(list(zip(self.dense_nets, self.batch_norms, self.dropouts))):\n",
    "            dense_net, batch_norm, dropout = nets\n",
    "            if i < len(self.dense_nets) - 1:\n",
    "                x = dropout(x)\n",
    "                x = dense_net(x)\n",
    "                x = self.activation(x)\n",
    "                x = batch_norm(x)\n",
    "            else:\n",
    "                x = dropout(x)\n",
    "                x =  dense_net(x)\n",
    "                #x = batch_norm(x)\n",
    "                #x = dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MKRankGaussScalar(object):\n",
    "    \"\"\"usage: \n",
    "    rgs = RankGaussScalar()\n",
    "    rgs.fit(df_X)\n",
    "    df_X_converted = rgs.transform(df_X)\n",
    "    df_X_test_converted = rgs.transform(df_X_test)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.fit_done = False\n",
    "\n",
    "    def rank_gauss(self, x):\n",
    "        N = x.shape[0]\n",
    "        temp = x.argsort()\n",
    "        rank_x = temp.argsort() / N\n",
    "        rank_x -= rank_x.mean()\n",
    "        rank_x *= 2\n",
    "        efi_x = erfinv(rank_x)\n",
    "        efi_x -= efi_x.mean()\n",
    "        return efi_x\n",
    "\n",
    "    def fit(self, df_x):\n",
    "        \"\"\"\n",
    "        df_x: fitting対象のDataFrame\n",
    "        \"\"\"\n",
    "        self.train_unique_rankgauss = {}\n",
    "        self.target_cols = np.sort(df_x.columns)\n",
    "        for c in self.target_cols:\n",
    "            unique_val = np.sort(df_x[c].unique())\n",
    "            self.train_unique_rankgauss[c]= [unique_val, self.rank_gauss(unique_val)]\n",
    "        self.fit_done = True\n",
    "\n",
    "    def transform(self, df_target):\n",
    "        \"\"\"\n",
    "        df_target: transform対象のDataFrame\n",
    "        \"\"\"\n",
    "        assert self.fit_done\n",
    "        assert np.all(np.sort(np.intersect1d(df_target.columns, self.target_cols)) == np.sort(self.target_cols))\n",
    "        df_converted_rank_gauss = pd.DataFrame(index=df_target.index)\n",
    "        for c in self.target_cols:\n",
    "            df_converted_rank_gauss[c] = np.interp(df_target[c], \n",
    "                                                   self.train_unique_rankgauss[c][0], \n",
    "                                                   self.train_unique_rankgauss[c][1]) # ,left=0, right=0)\n",
    "        return df_converted_rank_gauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "def worker_init_fn(worker_id):                                                          \n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "\n",
    "class plasticcDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.from_numpy(x.astype(np.float32))\n",
    "#        self.y = torch.from_numpy(y.astype(np.float32))\n",
    "        self.y = torch.LongTensor(y.astype(np.float32))\n",
    "        #self.y = y\n",
    "#        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    " \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx, :], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective\n",
    "# y_h に nan が起こりうる -> classsize が　0 になりうるのでどうにかせねば...\n",
    "# \n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
    "class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "def wloss_metric(preds, train_data):\n",
    "    weight_tensor = torch.tensor(np.array(list(class_weight.values())).astype(np.float), requires_grad=True).type(torch.FloatTensor)\n",
    "    #weight_tensor = torch.tensor(list(class_weight.values()), requires_grad=True).type(torch.FloatTensor)\n",
    "    y_t = torch.tensor(train_data, requires_grad=True).type(torch.LongTensor)\n",
    "    y_h = torch.zeros(y_t.shape[0], len(classes), requires_grad=True).scatter(1, y_t.reshape(-1, 1), 1)\n",
    "    print(y_t.shape)\n",
    "    y_h[y_h == 0] = 1\n",
    "    y_h /= y_h.sum(dim=0, keepdim=True)\n",
    "    y_p = torch.tensor(preds, requires_grad=True).type(torch.FloatTensor)\n",
    "    if len(y_p.shape) == 1:\n",
    "        y_p = y_p.reshape(len(classes), -1).transpose(0, 1)\n",
    "    ln_p = torch.log_softmax(y_p, dim=1)\n",
    "    wll = torch.sum(y_h * ln_p, dim=0)\n",
    "    loss = -torch.dot(weight_tensor, wll) / torch.sum(weight_tensor)\n",
    "    return loss\n",
    "\n",
    "def mywloss(y_pred_raw, y_true):  \n",
    "    weight_tensor = torch.tensor(np.array(list(class_weight.values())).astype(np.float), requires_grad=True).type(torch.FloatTensor)\n",
    "    y_onehot = torch.FloatTensor(y_true.shape[0], len(classes))\n",
    "    y_onehot.zero_()\n",
    "    y_onehot.scatter_(1, y_true.reshape(-1, 1), 1)\n",
    "    \n",
    "    y_pred_log=torch.log_softmax(y_pred_raw, dim=1)\n",
    "    class_num = torch.sum(y_onehot, dim=0)\n",
    "    class_num[class_num == 0] = 1\n",
    "    loss=-(torch.mean(torch.sum(torch.sum(y_onehot*y_pred_log, dim=0)*weight_tensor/class_num) / torch.sum(weight_tensor)))\n",
    "    return loss\n",
    "\n",
    "def mywloss2(y_pred_raw, y_true):  \n",
    "    weight_tensor = torch.tensor(np.array(list(class_weight.values())).astype(np.float), requires_grad=True).type(torch.FloatTensor)\n",
    "    y_onehot = torch.FloatTensor(y_true.shape[0], len(classes))\n",
    "    y_onehot.zero_()\n",
    "    y_onehot.scatter_(1, y_true.reshape(-1, 1), 1)\n",
    "    \n",
    "    y_pred_log=torch.log_softmax(y_pred_raw, dim=1)\n",
    "    class_num = torch.sum(y_onehot, dim=0)\n",
    "    class_num[class_num == 0] = 1\n",
    "######    #loss=-(torch.mean(torch.sum(torch.sum(y_onehot*y_pred_log, dim=0)*weight_tensor/class_num) / torch.sum(weight_tensor)))\n",
    "    loss=-(torch.mean(torch.sum(torch.sum(y_onehot*y_pred_log, dim=0)*weight_tensor)/torch.sum(weight_tensor)))\n",
    "    return loss\n",
    "\n",
    "def get_l2_loss(model, weight=0.1):\n",
    "    l2_weight = torch.tensor(weight)\n",
    "    l2_reg = torch.tensor(0.)\n",
    "    for param in model.parameters():\n",
    "        l2_reg += torch.norm(param)\n",
    "    return l2_weight * l2_reg\n",
    "\n",
    "def calc_team_score(y_true, y_preds):\n",
    "    '''\n",
    "    y_true:\u001b$B#1<!85$N\u001b(Bnp.array\n",
    "    y_pred:softmax\u001b$B8e$N#1\u001b(B4\u001b$B<!85$N\u001b(Bnp.array\n",
    "    '''\n",
    "    y_preds =  torch.softmax(y_preds, dim=1).detach().numpy()\n",
    "    \n",
    "    class99_prob = 1/9\n",
    "    class99_weight = 2\n",
    "            \n",
    "    y_p = y_preds * (1-class99_prob)\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    y_p_log = np.log(y_p)\n",
    "    \n",
    "    y_true_ohe = pd.get_dummies(y_true).values\n",
    "    nb_pos = y_true_ohe.sum(axis=0).astype(float)\n",
    "    \n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    \n",
    "    y_log_ones = np.sum(y_true_ohe * y_p_log, axis=0)\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "    score = - np.sum(y_w) / (np.sum(class_arr)+class99_weight)\\\n",
    "        + (class99_weight/(np.sum(class_arr)+class99_weight))*(-np.log(class99_prob))\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 0 =========\n",
      "========== 1 =========\n",
      "========== 2 =========\n",
      "========== 3 =========\n",
      "========== 4 =========\n",
      "========== 5 =========\n",
      "CPU times: user 22min 7s, sys: 14min 30s, total: 36min 37s\n",
      "Wall time: 32min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#imp_df = pd.read_csv('../importances/Booster_weight-multi-logloss-0.551369_2018-12-09-13-05-52_importance.csv')\n",
    "#imp_df = pd.read_csv('../importances/Booster_weight-multi-logloss-0.495417_2018-12-14-18-47-29_importance.csv')\n",
    "print('========== 0 =========')\n",
    "#imp_df = pd.read_csv('../importances/Booster_weight-multi-logloss-0.534367_2018-12-15-18-49-06_importance.csv')\n",
    "imp_df = pd.read_csv('../importances/Booster_weight-multi-logloss-0.581929_2018-12-26-13-15-24_importance.csv')\n",
    "FEATURES_TO_USE = imp_df.head(120).feature.tolist()\n",
    "\n",
    "print('========== 1 =========')\n",
    "train_df = pd.read_feather('../features/train/meta_features.ftr', nthreads=NTHREAD)\n",
    "test_df = pd.read_feather('../features/test/meta_features.ftr', nthreads=NTHREAD)\n",
    "#train_df = pd.read_pickle('../features/onodera_feats/X_train_0_1217-1.pkl.gz', compression='gzip')\n",
    "#test_df = pd.read_pickle('../features/onodera_feats/X_test_0_1217-1.pkl.gz', compression='gzip')\n",
    "\n",
    "print('========== 2 =========')\n",
    "# inf の変換\n",
    "train_df = train_df.replace(np.inf, np.nan)\n",
    "train_df = train_df.replace(-np.inf, np.nan)\n",
    "\n",
    "test_df = test_df.replace(np.inf, np.nan)\n",
    "test_df = test_df.replace(-np.inf, np.nan)\n",
    "\n",
    "#train_df = train_df[FEATURES_TO_USE]\n",
    "#test_df = test_df[FEATURES_TO_USE]\n",
    "train_df = train_df[list(set(test_df.columns.tolist()) & set(train_df.columns.tolist()))]\n",
    "\n",
    "print('========== 3 =========')\n",
    "mkscaler = MKRankGaussScalar()\n",
    "\n",
    "print('========== 4 =========')\n",
    "mkscaler.fit(train_df)\n",
    "x_train = mkscaler.transform(train_df).values\n",
    "x_test = mkscaler.transform(test_df).values\n",
    "\n",
    "print('========== 5 =========')\n",
    "x_train_scaled = x_train\n",
    "x_train_scaled = pd.DataFrame(x_train_scaled).fillna(0.).values\n",
    "gc.collect()\n",
    "\n",
    "# 欠損値埋め\n",
    "x_test_scaled = x_test\n",
    "x_test_scaled = pd.DataFrame(x_test_scaled).fillna(0.).values\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271148, 120)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271148, 1605)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e284189cfe534f829538e5a563f11619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 lr: 0.01000 train_loss : 0.75159 val_loss : 0.60484 akiyama-metric0.88646\n",
      "epoch : 1 lr: 0.01000 train_loss : 0.57259 val_loss : 0.53308 akiyama-metric0.82268\n",
      "epoch : 2 lr: 0.01000 train_loss : 0.52658 val_loss : 0.51688 akiyama-metric0.80828\n",
      "epoch : 3 lr: 0.01000 train_loss : 0.50022 val_loss : 0.49199 akiyama-metric0.78616\n",
      "epoch : 4 lr: 0.01000 train_loss : 0.47760 val_loss : 0.49634 akiyama-metric0.78616\n",
      "epoch : 5 lr: 0.01000 train_loss : 0.45862 val_loss : 0.47455 akiyama-metric0.77065\n",
      "epoch : 6 lr: 0.01000 train_loss : 0.44424 val_loss : 0.48664 akiyama-metric0.77065\n",
      "epoch : 7 lr: 0.01000 train_loss : 0.43716 val_loss : 0.47020 akiyama-metric0.76678\n",
      "epoch : 8 lr: 0.01000 train_loss : 0.42198 val_loss : 0.48117 akiyama-metric0.76678\n",
      "epoch : 9 lr: 0.01000 train_loss : 0.41542 val_loss : 0.46013 akiyama-metric0.75784\n",
      "epoch : 10 lr: 0.01000 train_loss : 0.40991 val_loss : 0.46182 akiyama-metric0.75784\n",
      "epoch : 11 lr: 0.01000 train_loss : 0.40265 val_loss : 0.45774 akiyama-metric0.75571\n",
      "epoch : 12 lr: 0.01000 train_loss : 0.39444 val_loss : 0.45886 akiyama-metric0.75571\n",
      "epoch : 13 lr: 0.01000 train_loss : 0.38700 val_loss : 0.46185 akiyama-metric0.75571\n",
      "epoch : 14 lr: 0.01000 train_loss : 0.38234 val_loss : 0.45739 akiyama-metric0.75540\n",
      "epoch : 15 lr: 0.01000 train_loss : 0.37798 val_loss : 0.46191 akiyama-metric0.75540\n",
      "epoch : 16 lr: 0.01000 train_loss : 0.36921 val_loss : 0.46542 akiyama-metric0.75540\n",
      "epoch : 17 lr: 0.01000 train_loss : 0.36784 val_loss : 0.45318 akiyama-metric0.75166\n",
      "epoch : 18 lr: 0.01000 train_loss : 0.36228 val_loss : 0.45612 akiyama-metric0.75166\n",
      "epoch : 19 lr: 0.01000 train_loss : 0.36243 val_loss : 0.45541 akiyama-metric0.75166\n",
      "epoch : 20 lr: 0.01000 train_loss : 0.35777 val_loss : 0.45307 akiyama-metric0.75156\n",
      "epoch : 21 lr: 0.01000 train_loss : 0.35513 val_loss : 0.45792 akiyama-metric0.75156\n",
      "epoch : 22 lr: 0.01000 train_loss : 0.35112 val_loss : 0.47193 akiyama-metric0.75156\n",
      "epoch : 23 lr: 0.01000 train_loss : 0.34665 val_loss : 0.45117 akiyama-metric0.74987\n",
      "epoch : 24 lr: 0.01000 train_loss : 0.34123 val_loss : 0.46838 akiyama-metric0.74987\n",
      "epoch : 25 lr: 0.01000 train_loss : 0.34313 val_loss : 0.45728 akiyama-metric0.74987\n",
      "epoch : 26 lr: 0.01000 train_loss : 0.33749 val_loss : 0.46212 akiyama-metric0.74987\n",
      "epoch : 27 lr: 0.01000 train_loss : 0.33800 val_loss : 0.45948 akiyama-metric0.74987\n",
      "epoch : 28 lr: 0.01000 train_loss : 0.33380 val_loss : 0.45497 akiyama-metric0.74987\n",
      "epoch : 29 lr: 0.01000 train_loss : 0.33035 val_loss : 0.46554 akiyama-metric0.74987\n",
      "epoch : 30 lr: 0.01000 train_loss : 0.33242 val_loss : 0.45741 akiyama-metric0.74987\n",
      "epoch : 31 lr: 0.01000 train_loss : 0.32821 val_loss : 0.46107 akiyama-metric0.74987\n",
      "epoch : 32 lr: 0.01000 train_loss : 0.32306 val_loss : 0.47099 akiyama-metric0.74987\n",
      "epoch : 33 lr: 0.01000 train_loss : 0.32893 val_loss : 0.47767 akiyama-metric0.74987\n",
      "epoch : 34 lr: 0.01000 train_loss : 0.32232 val_loss : 0.48146 akiyama-metric0.74987\n",
      "epoch : 35 lr: 0.01000 train_loss : 0.32119 val_loss : 0.56220 akiyama-metric0.74987\n",
      "epoch : 36 lr: 0.01000 train_loss : 0.31903 val_loss : 0.48254 akiyama-metric0.74987\n",
      "epoch : 37 lr: 0.01000 train_loss : 0.31695 val_loss : 0.46871 akiyama-metric0.74987\n",
      "epoch : 38 lr: 0.01000 train_loss : 0.31180 val_loss : 0.45470 akiyama-metric0.74987\n",
      "epoch : 39 lr: 0.01000 train_loss : 0.31252 val_loss : 0.46960 akiyama-metric0.74987\n",
      "epoch : 40 lr: 0.01000 train_loss : 0.31326 val_loss : 0.46351 akiyama-metric0.74987\n",
      "epoch : 41 lr: 0.01000 train_loss : 0.31210 val_loss : 0.46485 akiyama-metric0.74987\n",
      "epoch : 42 lr: 0.01000 train_loss : 0.31052 val_loss : 0.46696 akiyama-metric0.74987\n",
      "epoch : 43 lr: 0.01000 train_loss : 0.30830 val_loss : 0.46461 akiyama-metric0.74987\n",
      "epoch : 44 lr: 0.01000 train_loss : 0.30857 val_loss : 0.47274 akiyama-metric0.74987\n",
      "epoch : 45 lr: 0.01000 train_loss : 0.30692 val_loss : 0.45041 akiyama-metric0.74919\n",
      "epoch : 46 lr: 0.01000 train_loss : 0.30471 val_loss : 0.46768 akiyama-metric0.74919\n",
      "epoch : 47 lr: 0.01000 train_loss : 0.31179 val_loss : 0.45870 akiyama-metric0.74919\n",
      "epoch : 48 lr: 0.01000 train_loss : 0.30309 val_loss : 0.45669 akiyama-metric0.74919\n",
      "epoch : 49 lr: 0.01000 train_loss : 0.30223 val_loss : 0.47483 akiyama-metric0.74919\n",
      "epoch : 50 lr: 0.01000 train_loss : 0.29881 val_loss : 0.49960 akiyama-metric0.74919\n",
      "epoch : 51 lr: 0.01000 train_loss : 0.30103 val_loss : 0.44861 akiyama-metric0.74760\n",
      "epoch : 52 lr: 0.01000 train_loss : 0.29837 val_loss : 0.47427 akiyama-metric0.74760\n",
      "epoch : 53 lr: 0.01000 train_loss : 0.29548 val_loss : 0.47345 akiyama-metric0.74760\n",
      "epoch : 54 lr: 0.01000 train_loss : 0.30119 val_loss : 0.46198 akiyama-metric0.74760\n",
      "epoch : 55 lr: 0.01000 train_loss : 0.29730 val_loss : 0.46503 akiyama-metric0.74760\n",
      "epoch : 56 lr: 0.01000 train_loss : 0.29529 val_loss : 0.46247 akiyama-metric0.74760\n",
      "epoch : 57 lr: 0.01000 train_loss : 0.29759 val_loss : 0.46404 akiyama-metric0.74760\n",
      "epoch : 58 lr: 0.01000 train_loss : 0.29559 val_loss : 0.47428 akiyama-metric0.74760\n",
      "epoch : 59 lr: 0.01000 train_loss : 0.29859 val_loss : 0.46940 akiyama-metric0.74760\n",
      "epoch : 60 lr: 0.01000 train_loss : 0.29437 val_loss : 0.47912 akiyama-metric0.74760\n",
      "epoch : 61 lr: 0.01000 train_loss : 0.29122 val_loss : 0.46666 akiyama-metric0.74760\n",
      "epoch : 62 lr: 0.01000 train_loss : 0.29101 val_loss : 0.46922 akiyama-metric0.74760\n",
      "epoch : 63 lr: 0.01000 train_loss : 0.28804 val_loss : 0.46722 akiyama-metric0.74760\n",
      "epoch : 64 lr: 0.01000 train_loss : 0.29134 val_loss : 0.46762 akiyama-metric0.74760\n",
      "epoch : 65 lr: 0.01000 train_loss : 0.29261 val_loss : 0.46337 akiyama-metric0.74760\n",
      "epoch : 66 lr: 0.01000 train_loss : 0.28768 val_loss : 0.46830 akiyama-metric0.74760\n",
      "epoch : 67 lr: 0.01000 train_loss : 0.28796 val_loss : 0.46544 akiyama-metric0.74760\n",
      "epoch : 68 lr: 0.01000 train_loss : 0.28528 val_loss : 0.49129 akiyama-metric0.74760\n",
      "epoch : 69 lr: 0.01000 train_loss : 0.28675 val_loss : 0.46721 akiyama-metric0.74760\n",
      "epoch : 70 lr: 0.01000 train_loss : 0.28804 val_loss : 0.46681 akiyama-metric0.74760\n",
      "epoch : 71 lr: 0.01000 train_loss : 0.28261 val_loss : 0.45515 akiyama-metric0.74760\n",
      "epoch : 72 lr: 0.01000 train_loss : 0.28585 val_loss : 0.46771 akiyama-metric0.74760\n",
      "epoch : 73 lr: 0.01000 train_loss : 0.28361 val_loss : 0.48824 akiyama-metric0.74760\n",
      "epoch : 74 lr: 0.01000 train_loss : 0.28268 val_loss : 0.47950 akiyama-metric0.74760\n",
      "epoch : 75 lr: 0.01000 train_loss : 0.28260 val_loss : 0.48256 akiyama-metric0.74760\n",
      "epoch : 76 lr: 0.01000 train_loss : 0.28515 val_loss : 0.49825 akiyama-metric0.74760\n",
      "epoch : 77 lr: 0.01000 train_loss : 0.28296 val_loss : 0.47114 akiyama-metric0.74760\n",
      "epoch : 78 lr: 0.01000 train_loss : 0.28063 val_loss : 0.47036 akiyama-metric0.74760\n",
      "epoch : 79 lr: 0.01000 train_loss : 0.27880 val_loss : 0.47768 akiyama-metric0.74760\n",
      "epoch : 80 lr: 0.01000 train_loss : 0.28191 val_loss : 0.45158 akiyama-metric0.74760\n",
      "epoch : 81 lr: 0.01000 train_loss : 0.28161 val_loss : 0.47772 akiyama-metric0.74760\n",
      "best epoch: 51 best training loss 0.30103 best val: 0.44861 best team val: 0.74760\n",
      "epoch : 0 lr: 0.01000 train_loss : 0.75036 val_loss : 0.58464 akiyama-metric0.86851\n",
      "epoch : 1 lr: 0.01000 train_loss : 0.57708 val_loss : 0.55217 akiyama-metric0.83965\n",
      "epoch : 2 lr: 0.01000 train_loss : 0.52624 val_loss : 0.54797 akiyama-metric0.83592\n",
      "epoch : 3 lr: 0.01000 train_loss : 0.49749 val_loss : 0.51212 akiyama-metric0.80405\n",
      "epoch : 4 lr: 0.01000 train_loss : 0.47910 val_loss : 0.49702 akiyama-metric0.79063\n",
      "epoch : 5 lr: 0.01000 train_loss : 0.45961 val_loss : 0.50290 akiyama-metric0.79063\n",
      "epoch : 6 lr: 0.01000 train_loss : 0.44649 val_loss : 0.49247 akiyama-metric0.78658\n",
      "epoch : 7 lr: 0.01000 train_loss : 0.43600 val_loss : 0.51221 akiyama-metric0.78658\n",
      "epoch : 8 lr: 0.01000 train_loss : 0.42354 val_loss : 0.48699 akiyama-metric0.78171\n",
      "epoch : 9 lr: 0.01000 train_loss : 0.41582 val_loss : 0.48179 akiyama-metric0.77709\n",
      "epoch : 10 lr: 0.01000 train_loss : 0.40734 val_loss : 0.50283 akiyama-metric0.77709\n",
      "epoch : 11 lr: 0.01000 train_loss : 0.40213 val_loss : 0.46754 akiyama-metric0.76442\n",
      "epoch : 12 lr: 0.01000 train_loss : 0.39443 val_loss : 0.48192 akiyama-metric0.76442\n",
      "epoch : 13 lr: 0.01000 train_loss : 0.38937 val_loss : 0.47815 akiyama-metric0.76442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 14 lr: 0.01000 train_loss : 0.38531 val_loss : 0.46194 akiyama-metric0.75944\n",
      "epoch : 15 lr: 0.01000 train_loss : 0.37750 val_loss : 0.49047 akiyama-metric0.75944\n",
      "epoch : 16 lr: 0.01000 train_loss : 0.37097 val_loss : 0.46692 akiyama-metric0.75944\n",
      "epoch : 17 lr: 0.01000 train_loss : 0.36586 val_loss : 0.46913 akiyama-metric0.75944\n",
      "epoch : 18 lr: 0.01000 train_loss : 0.36260 val_loss : 0.46898 akiyama-metric0.75944\n",
      "epoch : 19 lr: 0.01000 train_loss : 0.35878 val_loss : 0.47431 akiyama-metric0.75944\n",
      "epoch : 20 lr: 0.01000 train_loss : 0.35781 val_loss : 0.49185 akiyama-metric0.75944\n",
      "epoch : 21 lr: 0.01000 train_loss : 0.35175 val_loss : 0.48184 akiyama-metric0.75944\n",
      "epoch : 22 lr: 0.01000 train_loss : 0.35082 val_loss : 0.46458 akiyama-metric0.75944\n",
      "epoch : 23 lr: 0.01000 train_loss : 0.34841 val_loss : 0.47957 akiyama-metric0.75944\n",
      "epoch : 24 lr: 0.01000 train_loss : 0.34072 val_loss : 0.47210 akiyama-metric0.75944\n",
      "epoch : 25 lr: 0.01000 train_loss : 0.34428 val_loss : 0.47870 akiyama-metric0.75944\n",
      "epoch : 26 lr: 0.01000 train_loss : 0.33831 val_loss : 0.48614 akiyama-metric0.75944\n",
      "epoch : 27 lr: 0.01000 train_loss : 0.33346 val_loss : 0.45922 akiyama-metric0.75703\n",
      "epoch : 28 lr: 0.01000 train_loss : 0.33344 val_loss : 0.47490 akiyama-metric0.75703\n",
      "epoch : 29 lr: 0.01000 train_loss : 0.33412 val_loss : 0.47920 akiyama-metric0.75703\n",
      "epoch : 30 lr: 0.01000 train_loss : 0.32749 val_loss : 0.46189 akiyama-metric0.75703\n",
      "epoch : 31 lr: 0.01000 train_loss : 0.32716 val_loss : 0.48871 akiyama-metric0.75703\n",
      "epoch : 32 lr: 0.01000 train_loss : 0.32613 val_loss : 0.48384 akiyama-metric0.75703\n",
      "epoch : 33 lr: 0.01000 train_loss : 0.32535 val_loss : 0.45283 akiyama-metric0.75135\n",
      "epoch : 34 lr: 0.01000 train_loss : 0.32205 val_loss : 0.49037 akiyama-metric0.75135\n",
      "epoch : 35 lr: 0.01000 train_loss : 0.32267 val_loss : 0.46979 akiyama-metric0.75135\n",
      "epoch : 36 lr: 0.01000 train_loss : 0.32153 val_loss : 0.47278 akiyama-metric0.75135\n",
      "epoch : 37 lr: 0.01000 train_loss : 0.31702 val_loss : 0.49042 akiyama-metric0.75135\n",
      "epoch : 38 lr: 0.01000 train_loss : 0.31394 val_loss : 0.48168 akiyama-metric0.75135\n",
      "epoch : 39 lr: 0.01000 train_loss : 0.31746 val_loss : 0.48274 akiyama-metric0.75135\n",
      "epoch : 40 lr: 0.01000 train_loss : 0.31270 val_loss : 0.48646 akiyama-metric0.75135\n",
      "epoch : 41 lr: 0.01000 train_loss : 0.31325 val_loss : 0.46687 akiyama-metric0.75135\n",
      "epoch : 42 lr: 0.01000 train_loss : 0.31015 val_loss : 0.49534 akiyama-metric0.75135\n",
      "epoch : 43 lr: 0.01000 train_loss : 0.30793 val_loss : 0.47560 akiyama-metric0.75135\n",
      "epoch : 44 lr: 0.01000 train_loss : 0.30841 val_loss : 0.48019 akiyama-metric0.75135\n",
      "epoch : 45 lr: 0.01000 train_loss : 0.30699 val_loss : 0.48193 akiyama-metric0.75135\n",
      "epoch : 46 lr: 0.01000 train_loss : 0.30588 val_loss : 0.48766 akiyama-metric0.75135\n",
      "epoch : 47 lr: 0.01000 train_loss : 0.30525 val_loss : 0.47969 akiyama-metric0.75135\n",
      "epoch : 48 lr: 0.01000 train_loss : 0.30424 val_loss : 0.46979 akiyama-metric0.75135\n",
      "epoch : 49 lr: 0.01000 train_loss : 0.29992 val_loss : 0.47551 akiyama-metric0.75135\n",
      "epoch : 50 lr: 0.01000 train_loss : 0.30142 val_loss : 0.47525 akiyama-metric0.75135\n",
      "epoch : 51 lr: 0.01000 train_loss : 0.29615 val_loss : 0.47411 akiyama-metric0.75135\n",
      "epoch : 52 lr: 0.01000 train_loss : 0.29950 val_loss : 0.47502 akiyama-metric0.75135\n",
      "epoch : 53 lr: 0.01000 train_loss : 0.29985 val_loss : 0.45931 akiyama-metric0.75135\n",
      "epoch : 54 lr: 0.01000 train_loss : 0.30031 val_loss : 0.47544 akiyama-metric0.75135\n",
      "epoch : 55 lr: 0.01000 train_loss : 0.29902 val_loss : 0.48429 akiyama-metric0.75135\n",
      "epoch : 56 lr: 0.01000 train_loss : 0.29940 val_loss : 0.48894 akiyama-metric0.75135\n",
      "epoch : 57 lr: 0.01000 train_loss : 0.29470 val_loss : 0.47707 akiyama-metric0.75135\n",
      "epoch : 58 lr: 0.01000 train_loss : 0.29487 val_loss : 0.49034 akiyama-metric0.75135\n",
      "epoch : 59 lr: 0.01000 train_loss : 0.29586 val_loss : 0.47201 akiyama-metric0.75135\n",
      "epoch : 60 lr: 0.01000 train_loss : 0.29590 val_loss : 0.48813 akiyama-metric0.75135\n",
      "epoch : 61 lr: 0.01000 train_loss : 0.29205 val_loss : 0.48208 akiyama-metric0.75135\n",
      "epoch : 62 lr: 0.01000 train_loss : 0.29237 val_loss : 0.48027 akiyama-metric0.75135\n",
      "epoch : 63 lr: 0.01000 train_loss : 0.29154 val_loss : 0.47992 akiyama-metric0.75135\n",
      "best epoch: 33 best training loss 0.32535 best val: 0.45283 best team val: 0.75135\n",
      "epoch : 0 lr: 0.01000 train_loss : 0.73964 val_loss : 0.58960 akiyama-metric0.87292\n",
      "epoch : 1 lr: 0.01000 train_loss : 0.56817 val_loss : 0.53787 akiyama-metric0.82694\n",
      "epoch : 2 lr: 0.01000 train_loss : 0.52142 val_loss : 0.51694 akiyama-metric0.80834\n",
      "epoch : 3 lr: 0.01000 train_loss : 0.50048 val_loss : 0.49632 akiyama-metric0.79001\n",
      "epoch : 4 lr: 0.01000 train_loss : 0.47307 val_loss : 0.47947 akiyama-metric0.77502\n",
      "epoch : 5 lr: 0.01000 train_loss : 0.45596 val_loss : 0.48391 akiyama-metric0.77502\n",
      "epoch : 6 lr: 0.01000 train_loss : 0.44414 val_loss : 0.47078 akiyama-metric0.76730\n",
      "epoch : 7 lr: 0.01000 train_loss : 0.43463 val_loss : 0.47129 akiyama-metric0.76730\n",
      "epoch : 8 lr: 0.01000 train_loss : 0.42005 val_loss : 0.46772 akiyama-metric0.76458\n",
      "epoch : 9 lr: 0.01000 train_loss : 0.40727 val_loss : 0.50138 akiyama-metric0.76458\n",
      "epoch : 10 lr: 0.01000 train_loss : 0.40738 val_loss : 0.47313 akiyama-metric0.76458\n",
      "epoch : 11 lr: 0.01000 train_loss : 0.40028 val_loss : 0.46399 akiyama-metric0.76126\n",
      "epoch : 12 lr: 0.01000 train_loss : 0.39428 val_loss : 0.46327 akiyama-metric0.76063\n",
      "epoch : 13 lr: 0.01000 train_loss : 0.38151 val_loss : 0.46295 akiyama-metric0.76034\n",
      "epoch : 14 lr: 0.01000 train_loss : 0.37989 val_loss : 0.45724 akiyama-metric0.75527\n",
      "epoch : 15 lr: 0.01000 train_loss : 0.37554 val_loss : 0.47156 akiyama-metric0.75527\n",
      "epoch : 16 lr: 0.01000 train_loss : 0.36760 val_loss : 0.47769 akiyama-metric0.75527\n",
      "epoch : 17 lr: 0.01000 train_loss : 0.36634 val_loss : 0.47002 akiyama-metric0.75527\n",
      "epoch : 18 lr: 0.01000 train_loss : 0.36415 val_loss : 0.47803 akiyama-metric0.75527\n",
      "epoch : 19 lr: 0.01000 train_loss : 0.35780 val_loss : 0.46241 akiyama-metric0.75527\n",
      "epoch : 20 lr: 0.01000 train_loss : 0.35601 val_loss : 0.47678 akiyama-metric0.75527\n",
      "epoch : 21 lr: 0.01000 train_loss : 0.35241 val_loss : 0.48099 akiyama-metric0.75527\n",
      "epoch : 22 lr: 0.01000 train_loss : 0.34957 val_loss : 0.47444 akiyama-metric0.75527\n",
      "epoch : 23 lr: 0.01000 train_loss : 0.34341 val_loss : 0.48363 akiyama-metric0.75527\n",
      "epoch : 24 lr: 0.01000 train_loss : 0.34265 val_loss : 0.46417 akiyama-metric0.75527\n",
      "epoch : 25 lr: 0.01000 train_loss : 0.34053 val_loss : 0.49262 akiyama-metric0.75527\n",
      "epoch : 26 lr: 0.01000 train_loss : 0.33672 val_loss : 0.49126 akiyama-metric0.75527\n",
      "epoch : 27 lr: 0.01000 train_loss : 0.33217 val_loss : 0.48926 akiyama-metric0.75527\n",
      "epoch : 28 lr: 0.01000 train_loss : 0.33459 val_loss : 0.51370 akiyama-metric0.75527\n",
      "epoch : 29 lr: 0.01000 train_loss : 0.32837 val_loss : 0.48722 akiyama-metric0.75527\n",
      "epoch : 30 lr: 0.01000 train_loss : 0.32786 val_loss : 0.51956 akiyama-metric0.75527\n",
      "epoch : 31 lr: 0.01000 train_loss : 0.32766 val_loss : 0.46765 akiyama-metric0.75527\n",
      "epoch : 32 lr: 0.01000 train_loss : 0.32716 val_loss : 0.47618 akiyama-metric0.75527\n",
      "epoch : 33 lr: 0.01000 train_loss : 0.32022 val_loss : 0.48162 akiyama-metric0.75527\n",
      "epoch : 34 lr: 0.01000 train_loss : 0.32113 val_loss : 0.49071 akiyama-metric0.75527\n",
      "epoch : 35 lr: 0.01000 train_loss : 0.31635 val_loss : 0.50241 akiyama-metric0.75527\n",
      "epoch : 36 lr: 0.01000 train_loss : 0.31724 val_loss : 0.47589 akiyama-metric0.75527\n",
      "epoch : 37 lr: 0.01000 train_loss : 0.32025 val_loss : 0.49749 akiyama-metric0.75527\n",
      "epoch : 38 lr: 0.01000 train_loss : 0.31112 val_loss : 0.46637 akiyama-metric0.75527\n",
      "epoch : 39 lr: 0.01000 train_loss : 0.31583 val_loss : 0.48893 akiyama-metric0.75527\n",
      "epoch : 40 lr: 0.01000 train_loss : 0.31223 val_loss : 0.50085 akiyama-metric0.75527\n",
      "epoch : 41 lr: 0.01000 train_loss : 0.30767 val_loss : 0.49424 akiyama-metric0.75527\n",
      "epoch : 42 lr: 0.01000 train_loss : 0.30930 val_loss : 0.48676 akiyama-metric0.75527\n",
      "epoch : 43 lr: 0.01000 train_loss : 0.30812 val_loss : 0.50115 akiyama-metric0.75527\n",
      "epoch : 44 lr: 0.01000 train_loss : 0.30594 val_loss : 0.48664 akiyama-metric0.75527\n",
      "best epoch: 14 best training loss 0.37989 best val: 0.45724 best team val: 0.75527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 lr: 0.01000 train_loss : 0.74904 val_loss : 0.55357 akiyama-metric0.84090\n",
      "epoch : 1 lr: 0.01000 train_loss : 0.57221 val_loss : 0.52055 akiyama-metric0.81154\n",
      "epoch : 2 lr: 0.01000 train_loss : 0.52351 val_loss : 0.50430 akiyama-metric0.79710\n",
      "epoch : 3 lr: 0.01000 train_loss : 0.49606 val_loss : 0.50209 akiyama-metric0.79513\n",
      "epoch : 4 lr: 0.01000 train_loss : 0.47114 val_loss : 0.48630 akiyama-metric0.78110\n",
      "epoch : 5 lr: 0.01000 train_loss : 0.45710 val_loss : 0.48969 akiyama-metric0.78110\n",
      "epoch : 6 lr: 0.01000 train_loss : 0.44518 val_loss : 0.47290 akiyama-metric0.76919\n",
      "epoch : 7 lr: 0.01000 train_loss : 0.42633 val_loss : 0.47942 akiyama-metric0.76919\n",
      "epoch : 8 lr: 0.01000 train_loss : 0.42072 val_loss : 0.47676 akiyama-metric0.76919\n",
      "epoch : 9 lr: 0.01000 train_loss : 0.41006 val_loss : 0.46394 akiyama-metric0.76123\n",
      "epoch : 10 lr: 0.01000 train_loss : 0.40194 val_loss : 0.47051 akiyama-metric0.76123\n",
      "epoch : 11 lr: 0.01000 train_loss : 0.39581 val_loss : 0.47080 akiyama-metric0.76123\n",
      "epoch : 12 lr: 0.01000 train_loss : 0.39059 val_loss : 0.48218 akiyama-metric0.76123\n",
      "epoch : 13 lr: 0.01000 train_loss : 0.38162 val_loss : 0.46631 akiyama-metric0.76123\n",
      "epoch : 14 lr: 0.01000 train_loss : 0.38260 val_loss : 0.47560 akiyama-metric0.76123\n",
      "epoch : 15 lr: 0.01000 train_loss : 0.37349 val_loss : 0.46435 akiyama-metric0.76123\n",
      "epoch : 16 lr: 0.01000 train_loss : 0.36742 val_loss : 0.46099 akiyama-metric0.75860\n",
      "epoch : 17 lr: 0.01000 train_loss : 0.36321 val_loss : 0.46613 akiyama-metric0.75860\n",
      "epoch : 18 lr: 0.01000 train_loss : 0.35985 val_loss : 0.46484 akiyama-metric0.75860\n",
      "epoch : 19 lr: 0.01000 train_loss : 0.35328 val_loss : 0.46363 akiyama-metric0.75860\n",
      "epoch : 20 lr: 0.01000 train_loss : 0.35009 val_loss : 0.46954 akiyama-metric0.75860\n",
      "epoch : 21 lr: 0.01000 train_loss : 0.35039 val_loss : 0.45253 akiyama-metric0.75108\n",
      "epoch : 22 lr: 0.01000 train_loss : 0.34491 val_loss : 0.45924 akiyama-metric0.75108\n",
      "epoch : 23 lr: 0.01000 train_loss : 0.34076 val_loss : 0.46121 akiyama-metric0.75108\n",
      "epoch : 24 lr: 0.01000 train_loss : 0.34078 val_loss : 0.47461 akiyama-metric0.75108\n",
      "epoch : 25 lr: 0.01000 train_loss : 0.33790 val_loss : 0.46806 akiyama-metric0.75108\n",
      "epoch : 26 lr: 0.01000 train_loss : 0.33496 val_loss : 0.47523 akiyama-metric0.75108\n",
      "epoch : 27 lr: 0.01000 train_loss : 0.33205 val_loss : 0.48029 akiyama-metric0.75108\n",
      "epoch : 28 lr: 0.01000 train_loss : 0.33204 val_loss : 0.45865 akiyama-metric0.75108\n",
      "epoch : 29 lr: 0.01000 train_loss : 0.32834 val_loss : 0.47644 akiyama-metric0.75108\n",
      "epoch : 30 lr: 0.01000 train_loss : 0.32432 val_loss : 0.47933 akiyama-metric0.75108\n",
      "epoch : 31 lr: 0.01000 train_loss : 0.32345 val_loss : 0.47323 akiyama-metric0.75108\n",
      "epoch : 32 lr: 0.01000 train_loss : 0.32334 val_loss : 0.46807 akiyama-metric0.75108\n",
      "epoch : 33 lr: 0.01000 train_loss : 0.32047 val_loss : 0.47456 akiyama-metric0.75108\n",
      "epoch : 34 lr: 0.01000 train_loss : 0.31957 val_loss : 0.47341 akiyama-metric0.75108\n",
      "epoch : 35 lr: 0.01000 train_loss : 0.32199 val_loss : 0.48114 akiyama-metric0.75108\n",
      "epoch : 36 lr: 0.01000 train_loss : 0.31529 val_loss : 0.47845 akiyama-metric0.75108\n",
      "epoch : 37 lr: 0.01000 train_loss : 0.31386 val_loss : 0.46677 akiyama-metric0.75108\n",
      "epoch : 38 lr: 0.01000 train_loss : 0.31251 val_loss : 0.47369 akiyama-metric0.75108\n",
      "epoch : 39 lr: 0.01000 train_loss : 0.31243 val_loss : 0.47645 akiyama-metric0.75108\n",
      "epoch : 40 lr: 0.01000 train_loss : 0.31278 val_loss : 0.48151 akiyama-metric0.75108\n",
      "epoch : 41 lr: 0.01000 train_loss : 0.31168 val_loss : 0.48544 akiyama-metric0.75108\n",
      "epoch : 42 lr: 0.01000 train_loss : 0.30893 val_loss : 0.48727 akiyama-metric0.75108\n",
      "epoch : 43 lr: 0.01000 train_loss : 0.30671 val_loss : 0.46911 akiyama-metric0.75108\n",
      "epoch : 44 lr: 0.01000 train_loss : 0.30352 val_loss : 0.47448 akiyama-metric0.75108\n",
      "epoch : 45 lr: 0.01000 train_loss : 0.30545 val_loss : 0.47493 akiyama-metric0.75108\n",
      "epoch : 46 lr: 0.01000 train_loss : 0.30270 val_loss : 0.48749 akiyama-metric0.75108\n",
      "epoch : 47 lr: 0.01000 train_loss : 0.29830 val_loss : 0.47674 akiyama-metric0.75108\n",
      "epoch : 48 lr: 0.01000 train_loss : 0.29858 val_loss : 0.48420 akiyama-metric0.75108\n",
      "epoch : 49 lr: 0.01000 train_loss : 0.30095 val_loss : 0.47329 akiyama-metric0.75108\n",
      "epoch : 50 lr: 0.01000 train_loss : 0.29646 val_loss : 0.48119 akiyama-metric0.75108\n",
      "epoch : 51 lr: 0.01000 train_loss : 0.29339 val_loss : 0.48142 akiyama-metric0.75108\n",
      "best epoch: 21 best training loss 0.35039 best val: 0.45253 best team val: 0.75108\n",
      "epoch : 0 lr: 0.01000 train_loss : 0.73679 val_loss : 0.58580 akiyama-metric0.86954\n",
      "epoch : 1 lr: 0.01000 train_loss : 0.56560 val_loss : 0.59819 akiyama-metric0.86954\n",
      "epoch : 6 lr: 0.01000 train_loss : 0.44077 val_loss : 0.49633 akiyama-metric0.79002\n",
      "epoch : 7 lr: 0.01000 train_loss : 0.43301 val_loss : 0.48782 akiyama-metric0.78245\n",
      "epoch : 8 lr: 0.01000 train_loss : 0.41653 val_loss : 0.48174 akiyama-metric0.77704\n",
      "epoch : 9 lr: 0.01000 train_loss : 0.41364 val_loss : 0.48004 akiyama-metric0.77553\n",
      "epoch : 10 lr: 0.01000 train_loss : 0.40324 val_loss : 0.49987 akiyama-metric0.77553\n",
      "epoch : 11 lr: 0.01000 train_loss : 0.39300 val_loss : 0.48692 akiyama-metric0.77553\n",
      "epoch : 12 lr: 0.01000 train_loss : 0.38861 val_loss : 0.49712 akiyama-metric0.77553\n",
      "epoch : 13 lr: 0.01000 train_loss : 0.38160 val_loss : 0.47580 akiyama-metric0.77176\n",
      "epoch : 14 lr: 0.01000 train_loss : 0.37956 val_loss : 0.48964 akiyama-metric0.77176\n",
      "epoch : 15 lr: 0.01000 train_loss : 0.37109 val_loss : 0.48100 akiyama-metric0.77176\n",
      "epoch : 16 lr: 0.01000 train_loss : 0.36822 val_loss : 0.48137 akiyama-metric0.77176\n",
      "epoch : 17 lr: 0.01000 train_loss : 0.36689 val_loss : 0.47546 akiyama-metric0.77146\n",
      "epoch : 18 lr: 0.01000 train_loss : 0.35876 val_loss : 0.48344 akiyama-metric0.77146\n",
      "epoch : 19 lr: 0.01000 train_loss : 0.35580 val_loss : 0.47753 akiyama-metric0.77146\n",
      "epoch : 20 lr: 0.01000 train_loss : 0.35340 val_loss : 0.47237 akiyama-metric0.76872\n",
      "epoch : 21 lr: 0.01000 train_loss : 0.34815 val_loss : 0.48905 akiyama-metric0.76872\n",
      "epoch : 22 lr: 0.01000 train_loss : 0.34546 val_loss : 0.49199 akiyama-metric0.76872\n",
      "epoch : 23 lr: 0.01000 train_loss : 0.34343 val_loss : 0.47583 akiyama-metric0.76872\n",
      "epoch : 24 lr: 0.01000 train_loss : 0.34183 val_loss : 0.48758 akiyama-metric0.76872\n",
      "epoch : 25 lr: 0.01000 train_loss : 0.33605 val_loss : 0.49018 akiyama-metric0.76872\n",
      "epoch : 26 lr: 0.01000 train_loss : 0.33794 val_loss : 0.48246 akiyama-metric0.76872\n",
      "epoch : 27 lr: 0.01000 train_loss : 0.33373 val_loss : 0.49989 akiyama-metric0.76872\n",
      "epoch : 28 lr: 0.01000 train_loss : 0.33347 val_loss : 0.47889 akiyama-metric0.76872\n",
      "epoch : 29 lr: 0.01000 train_loss : 0.32801 val_loss : 0.48873 akiyama-metric0.76872\n",
      "epoch : 30 lr: 0.01000 train_loss : 0.32715 val_loss : 0.47165 akiyama-metric0.76808\n",
      "epoch : 31 lr: 0.01000 train_loss : 0.32535 val_loss : 0.49088 akiyama-metric0.76808\n",
      "epoch : 32 lr: 0.01000 train_loss : 0.32150 val_loss : 0.48751 akiyama-metric0.76808\n",
      "epoch : 33 lr: 0.01000 train_loss : 0.32067 val_loss : 0.49674 akiyama-metric0.76808\n",
      "epoch : 34 lr: 0.01000 train_loss : 0.31677 val_loss : 0.47775 akiyama-metric0.76808\n",
      "epoch : 35 lr: 0.01000 train_loss : 0.31857 val_loss : 0.48652 akiyama-metric0.76808\n",
      "epoch : 36 lr: 0.01000 train_loss : 0.31893 val_loss : 0.48731 akiyama-metric0.76808\n",
      "epoch : 37 lr: 0.01000 train_loss : 0.31470 val_loss : 0.51437 akiyama-metric0.76808\n",
      "epoch : 38 lr: 0.01000 train_loss : 0.31196 val_loss : 0.48660 akiyama-metric0.76808\n",
      "epoch : 39 lr: 0.01000 train_loss : 0.31216 val_loss : 0.50944 akiyama-metric0.76808\n",
      "epoch : 40 lr: 0.01000 train_loss : 0.30862 val_loss : 0.49175 akiyama-metric0.76808\n",
      "epoch : 41 lr: 0.01000 train_loss : 0.30754 val_loss : 0.49785 akiyama-metric0.76808\n",
      "epoch : 42 lr: 0.01000 train_loss : 0.30614 val_loss : 0.48601 akiyama-metric0.76808\n",
      "epoch : 43 lr: 0.01000 train_loss : 0.30459 val_loss : 0.51527 akiyama-metric0.76808\n",
      "epoch : 44 lr: 0.01000 train_loss : 0.30112 val_loss : 0.49084 akiyama-metric0.76808\n",
      "epoch : 45 lr: 0.01000 train_loss : 0.30133 val_loss : 0.48823 akiyama-metric0.76808\n",
      "epoch : 46 lr: 0.01000 train_loss : 0.30414 val_loss : 0.50539 akiyama-metric0.76808\n",
      "epoch : 47 lr: 0.01000 train_loss : 0.30400 val_loss : 0.52265 akiyama-metric0.76808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 48 lr: 0.01000 train_loss : 0.30629 val_loss : 0.49951 akiyama-metric0.76808\n",
      "epoch : 49 lr: 0.01000 train_loss : 0.30123 val_loss : 0.49114 akiyama-metric0.76808\n",
      "epoch : 50 lr: 0.01000 train_loss : 0.29978 val_loss : 0.48816 akiyama-metric0.76808\n",
      "epoch : 51 lr: 0.01000 train_loss : 0.29743 val_loss : 0.50988 akiyama-metric0.76808\n",
      "epoch : 52 lr: 0.01000 train_loss : 0.29802 val_loss : 0.48995 akiyama-metric0.76808\n",
      "epoch : 53 lr: 0.01000 train_loss : 0.29696 val_loss : 0.51274 akiyama-metric0.76808\n",
      "epoch : 54 lr: 0.01000 train_loss : 0.29623 val_loss : 0.51161 akiyama-metric0.76808\n",
      "epoch : 55 lr: 0.01000 train_loss : 0.29317 val_loss : 0.48958 akiyama-metric0.76808\n",
      "epoch : 56 lr: 0.01000 train_loss : 0.29269 val_loss : 0.48386 akiyama-metric0.76808\n",
      "epoch : 57 lr: 0.01000 train_loss : 0.29244 val_loss : 0.49288 akiyama-metric0.76808\n",
      "epoch : 58 lr: 0.01000 train_loss : 0.29095 val_loss : 0.49665 akiyama-metric0.76808\n",
      "epoch : 59 lr: 0.01000 train_loss : 0.28802 val_loss : 0.50477 akiyama-metric0.76808\n",
      "epoch : 60 lr: 0.01000 train_loss : 0.28660 val_loss : 0.51948 akiyama-metric0.76808\n",
      "best epoch: 30 best training loss 0.32715 best val: 0.47165 best team val: 0.76808\n",
      "\n",
      "===== 0.4565733075141907 =====\n",
      "saved oof to ../oof/PLASTICC_NET_weight-multi-logloss-0.456573.pkl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca9e9b5b8ec46b6a65ec302f24395b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "label_num = 14\n",
    "FOLD_NUM = 5\n",
    "DO_RATE = 0.5\n",
    "max_layer_size = 512\n",
    "#SEEDS = [71, 1000, 0, 99, 713]\n",
    "SEEDS = [71, ]\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "#le.fit(train_df['target'].values)\n",
    "#read_targets = pd.read_csv('/home/naoya.taguchi/.kaggle/competitions/PLAsTiCC-2018/training_set_metadata.csv')\n",
    "read_targets = pd.read_hdf('/home/naoya.taguchi/.kaggle/competitions/PLAsTiCC-2018/kyle_final_augment.h5', 'meta').reset_index()\n",
    "le.fit(read_targets['target'].values)\n",
    "\n",
    "x_train = x_train_scaled\n",
    "#y_train = le.transform(train_df.target)\n",
    "y_train = le.transform(read_targets.target)\n",
    "\n",
    "\n",
    "#seed = 90\n",
    "for seed in SEEDS:\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # skf = StratifiedKFold(n_splits=FOLD_NUM, shuffle=True, random_state=71)\n",
    "    # folds = skf.split(x_train, y_train)\n",
    "    folds = []\n",
    "    for fold in range(5):\n",
    "        _trn_idx = read_targets.query(f'fold != {fold}').index.tolist()\n",
    "        _val_idx = read_targets.query(f'fold == {fold}').index.tolist()\n",
    "        folds.append([_trn_idx, _val_idx])\n",
    "\n",
    "    best_models = []\n",
    "    best_scores = []\n",
    "    best_team_scores = []\n",
    "    best_losses = []\n",
    "    oof = []\n",
    "\n",
    "    for trn_idx, val_idx in tqdm(list(folds)):\n",
    "        x_trn, x_val = x_train[trn_idx], x_train[val_idx]\n",
    "        y_trn, y_val = y_train[trn_idx], y_train[val_idx]\n",
    "    \n",
    "        #mlp_unit_nums = (max_layer_size, max_layer_size//2, max_layer_size//4,  label_num)\n",
    "        mlp_unit_nums = (max_layer_size, max_layer_size//2, max_layer_size//4, max_layer_size//8, label_num)\n",
    "    \n",
    "        pnn = plasticcNet(input_unit_num=x_trn.shape[1], mlp_unit_nums=mlp_unit_nums, init_do_rate=DO_RATE*0.2, middle_do_rate=DO_RATE, last_do_rate=DO_RATE*0.5)\n",
    "        optimizer = optim.SGD(pnn.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4, nesterov=True)\n",
    "        # optimizer = optim.SGD(pnn.parameters(), lr=0.6, momentum=0.0)\n",
    "        # scheduler = StepLR(optimizer, step_size=5, gamma=0.6)\n",
    "            \n",
    "        dataset = plasticcDataset(x_trn, y_trn)\n",
    "        dataloader = DataLoader(dataset, batch_size=100, shuffle=True, num_workers=0, worker_init_fn=worker_init_fn)\n",
    "    \n",
    "        epochs = 1000\n",
    "        losses = []\n",
    "        best_val = 100000000\n",
    "        best_count = 0\n",
    "        best_model = None   \n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # scheduler.step()\n",
    "            for i_batch, sample_batched in enumerate(dataloader):\n",
    "                pnn = pnn.train()\n",
    "                optimizer.zero_grad()\n",
    "                x_trn_batch, y_trn_batch = sample_batched\n",
    "                x_trn_batch, y_trn_batch = Variable(x_trn_batch), Variable(y_trn_batch)\n",
    "                _res = pnn(x_trn_batch)\n",
    "                loss = mywloss(_res, y_trn_batch)\n",
    "                #loss = mywloss2(_res, y_trn_batch)\n",
    "                #loss = cross_entropy(_res, y_trn_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses.append(loss.detach().numpy())\n",
    "                #break\n",
    "            pnn = pnn.eval()\n",
    "            # oof いれる！！！！\n",
    "            val_res = pnn(torch.Tensor(x_val))\n",
    "            val_score = mywloss(val_res, torch.LongTensor(y_val)).detach().numpy()\n",
    "            #val_score = cross_entropy(val_res, torch.LongTensor(y_val)).detach().numpy()\n",
    "            val_team_score = calc_team_score(y_val, val_res)\n",
    "            if epoch % 1 == 0:\n",
    "                if best_val > val_score:\n",
    "                    best_epoch = epoch\n",
    "                    best_val = val_score\n",
    "                    best_team_val = val_team_score\n",
    "                    best_loss = np.mean(losses)\n",
    "                    best_count = 0\n",
    "                    torch.save(pnn.state_dict(), '../check_points/temp.pth')\n",
    "                else:\n",
    "                    best_count += 1\n",
    "                print(f'epoch : {epoch}', f'lr: {optimizer.state_dict()[\"param_groups\"][0][\"lr\"]:.5f}', f'train_loss : {np.mean(losses):.5f}', f'val_loss : {val_score:.5f}', f'akiyama-metric{best_team_val:.5f}')\n",
    "                losses = []\n",
    "                if best_count >= 30:\n",
    "                    print(f'best epoch: {best_epoch}', f'best training loss {best_loss:.5f}', f'best val: {best_val:.5f}', f'best team val: {best_team_val:.5f}')\n",
    "                    pnn.load_state_dict(torch.load('../check_points/temp.pth'))\n",
    "                    best_models.append(pnn)\n",
    "                    best_val_res = pnn(torch.Tensor(x_val))\n",
    "                    oof.append([best_val_res.detach().numpy(), y_val, val_idx])\n",
    "                    best_scores.append(best_val)\n",
    "                    best_team_scores.append(best_team_val)\n",
    "                    best_losses.append(best_loss)\n",
    "                    gc.collect()\n",
    "                    break\n",
    "            # break\n",
    "    #    break\n",
    "    \n",
    "    submission_file_name = '../submissions/PLASTICC_NET_weight-multi-logloss-{:.6}.csv'.format(np.mean(best_scores))\n",
    "    print(f'===== {np.mean(best_scores)} =====')\n",
    "    \n",
    "    oof_filename = submission_file_name.split('/')[-1][:-4]\n",
    "    oof_filename = '../oof/' + oof_filename + '.pkl'\n",
    "    with open(oof_filename, 'wb') as fout:\n",
    "        pickle.dump(oof, fout)\n",
    "    print(f'saved oof to {oof_filename}')   \n",
    "    \n",
    "    test_reses = []\n",
    "    for bpnn in tqdm(best_models):\n",
    "        # torch の nn の出力を ↑　の自作 softmax でさばくと nan ができてしまう\n",
    "        test_reses.append(torch.nn.functional.softmax(bpnn(torch.Tensor(x_test_scaled))).detach().numpy())\n",
    "    \n",
    "    res = np.clip(np.mean(test_reses, axis=0), 10**(-15), 1 - 10**(-15))\n",
    "    preds_99 = np.ones((res.shape[0]))\n",
    "    for i in range(res.shape[1]):\n",
    "        preds_99 *= (1 - res[:, i])\n",
    "    preds_99 = 0.14 * preds_99 / np.mean(preds_99)\n",
    "    \n",
    "    res_df = pd.DataFrame(res, columns=[\n",
    "        'class_6',\n",
    "        'class_15',\n",
    "        'class_16',\n",
    "        'class_42',\n",
    "        'class_52',\n",
    "        'class_53',\n",
    "        'class_62',\n",
    "        'class_64',\n",
    "        'class_65',\n",
    "        'class_67',\n",
    "        'class_88',\n",
    "        'class_90',\n",
    "        'class_92',\n",
    "        'class_95',\n",
    "    ])\n",
    "    res_df['class_99'] = preds_99\n",
    "    \n",
    "    object_ids = pd.read_csv('/home/naoya.taguchi/.kaggle/competitions/PLAsTiCC-2018/test_set_metadata.csv').object_id\n",
    "    pd.concat([object_ids, res_df], axis=1).to_csv(submission_file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle('../share/PLASTICC_NET_weight-multi-logloss-0.538699.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PLASTICC_NET_weight' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f6beab0278e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mPLASTICC_NET_weight\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmulti\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlogloss\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.59564\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpkl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'PLASTICC_NET_weight' is not defined"
     ]
    }
   ],
   "source": [
    "PLASTICC_NET_weight-multi-logloss-0.59564.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_df = pd.read_csv('../submissions/Booster_weight-multi-logloss-0.581929_2018-12-26-13-15-24.csv')\n",
    "nn_df = pd.read_csv('../submissions/PLASTICC_NET_weight-multi-logloss-0.59564.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "((lgb_df.set_index('object_id') + nn_df.set_index('object_id'))/2).to_csv('../submissions/nn_and_lgb_kyle.csv.gz', index=True, compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
